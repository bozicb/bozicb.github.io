is.data.frame(flights)
arrange(flights,desc(is.na(houst_2hdelay)))
flights
arrange(flights,desc(is.na(flights)))
arrange(flights,desc(is.na(flights$dep_time)))
arrange(flights,desc(is.na(flights)))
arrange(flights,desc(is.na(flights)))
arrange(flights,desc(is.na(dep_delay)))
#4
longest <- filter(flights, distance==max(distance))
View(longest)
View(longest)
distinct(longest, flight)
select(flights,contains('time'))
#5
select(flights,contains('time'))
#6
cancelled <- mutate(flights, cancelled = (is.na(arr_delay) | is.na(dep_delay)))
View(cancelled)
cancelled_by_day <- group_by(cancelled, year, month, day)
View(cancelled_by_day)
?summarise
cancelled_per_day <- summarise(cancelled_by_day, cancelled_num = sum(cancelled))
View(cancelled_by_day)
summarise(cancelled_by_day, cancelled_num = sum(cancelled))
cancelled_per_day <-
summarise(cancelled_by_day, cancelled_num = sum(cancelled), flights_num = n())
summarise(cancelled_by_day, cancelled_num = sum(cancelled), flights_num = n())
ggplot(cancelled_by_day) + geom_point(aes(x = flights_num, y = cancelled_num))
summarise(cancelled_by_day, cancelled_num = sum(cancelled), flights_num = n())
cancelled_per_day <- summarise(cancelled_by_day, cancelled_num = sum(cancelled), flights_num = n())
ggplot(cancelled_per_day) + geom_point(aes(x = flights_num, y = cancelled_num))
#7
carriers <- group_by(flights, carriers)
#7
carriers <- group_by(flights, carrier)
View(carriers)
carrier_delays <- summarise(carriers, arr_delay = mean(arr_delay, na.rm = TRUE))
summarise(carriers, arr_delay = mean(arr_delay, na.rm = TRUE))
worst_delays <- arrange(carrier_delays, desc(arr_delay))
View(worst_delays)
filter(airlines, carrier = worst_delays$carrier[1])
filter(airlines, carrier == worst_delays$carrier[1])
max(flights$dep_delay)
max(flights$dep_delay, na.rm=TRUE)
ans7_df <- subset( nycflights13::flights, subset= flights$dep_delay == max, na.rm = TRUE )
View(cancelled)
#8
plane_flights <- filter(flights, !is.na(tailnum))
planes <- group_by(plane_flights, tailnum)
planes_100 <- filter(count(planes), n >= 100)
View(planes_100)
airlines
#9
flights_by_hour <- group_by(flights, hour)
not_cancelled_by_hour <- filter(flights_by_hour, !is.na(dep_delay))
summarise(not_cancelled_by_hour, delay=mean(dep_delay>0, na.rm=T))
delays_by_hour <- summarise(not_cancelled_by_hour, delay=mean(dep_delay>0, na.rm=T))
ggplot(delays_by_hour, aes(hour, delay, fill=delay)) + geom_col()
#10
airport_locations <- airports %>%
select(faa, lat, lon)
airport_locations <- airports %>%
select(faa, lat, lon)
flights %>% select(year:day, hour, origin, dest) %>%
left_join(
airport_locations,
by = c("origin" = "faa")
) %>%
left_join(
airport_locations,
by = c("dest" = "faa"),
suffix = c("_origin", "_dest")
)
knitr::opts_chunk$set(echo = TRUE)
#We are using a .dat file (survey.dat) created from the SPSS file survey.sav  taken from SPSS Survival Manual 6th Edition Julie Pallant
#http://spss.allenandunwin.com.s3-website-ap-southeast-2.amazonaws.com/data-files.html#.Wb0vvnWP-po
#Results on a survey on well being
#We need to load the file so that we can use it in R.
survey <- read.table("../data/survey.dat")
#Setting the column names to be that used in the dataset
colnames(survey) <- tolower(colnames(survey))
library(pastecs) #For creating descriptive statistic summaries
library(ggplot2) #For creating histograms with more detail than plot
library(psych) # Some useful descriptive functions
library(semTools) #For skewness and kurtosis
library(car) # For Levene's test for homogeneity of variance
library(coin)# For Wilcox test (non-parametric)
#Get descriptive stastitics by group
describeBy(survey$tslfest, survey$sex)
#We are using a .dat file (survey.dat) created from the SPSS file survey.sav  taken from SPSS Survival Manual 6th Edition Julie Pallant
#http://spss.allenandunwin.com.s3-website-ap-southeast-2.amazonaws.com/data-files.html#.Wb0vvnWP-po
#Results on a survey on well being
#We need to load the file so that we can use it in R.
survey <- read.table("../data/survey.dat")
#Setting the column names to be that used in the dataset
colnames(survey) <- tolower(colnames(survey))
library(pastecs) #For creating descriptive statistic summaries
library(ggplot2) #For creating histograms with more detail than plot
library(psych) # Some useful descriptive functions
library(semTools) #For skewness and kurtosis
library(car) # For Levene's test for homogeneity of variance
library(coin)# For Wilcox test (non-parametric)
#Setting the column names to be that used in the dataset
colnames(survey) <- tolower(colnames(survey))
#We are using a .dat file (survey.dat) created from the SPSS file survey.sav  taken from SPSS Survival Manual 6th Edition Julie Pallant
#http://spss.allenandunwin.com.s3-website-ap-southeast-2.amazonaws.com/data-files.html#.Wb0vvnWP-po
#Results on a survey on well being
#We need to load the file so that we can use it in R.
survey <- read.table("../data/survey.dat")
#Setting the column names to be that used in the dataset
colnames(survey) <- tolower(colnames(survey))
library(pastecs) #For creating descriptive statistic summaries
library(ggplot2) #For creating histograms with more detail than plot
library(psych) # Some useful descriptive functions
library(semTools) #For skewness and kurtosis
library(car) # For Levene's test for homogeneity of variance
library(coin)# For Wilcox test (non-parametric)
#Get descriptive stastitics by group
describeBy(survey$tslfest, survey$sex)
#Conduct Levene's test for homogeneity of variance in library car
ltest<-car::leveneTest(tslfest ~ sex, data=survey)
#Pr(F) is your probability
ltest
#Conduct the t-test from package stats
#You can use the var.equal = TRUE option to specify equal variances and a pooled variance estimate
stats::t.test(tslfest~sex,var.equal=TRUE,data=survey)
knitr::opts_chunk$set(echo = TRUE)
#Get descriptive stastitics by group
#describeBy is part of the psych package so you need to use it
describeBy(regression$normexam,group=regression$girl)
#Read in the file
regression <- read.spss("../data/Regression.sav", use.value.labels=TRUE, max.value.labels=Inf, to.data.frame=TRUE)
#Inspecting the data
#Remember to install these packages if you haven't already done so
library(foreign)
library(pastecs) #For creating descriptive statistic summaries
library(ggplot2) #For creating histograms with more detail than plot
library(psych) # Some useful descriptive functions
library(semTools) #For skewness and kurtosis
library(car) # For Levene's test for homogeneity of variance
library(coin) #for Wilcoxon test
#Read in the file
regression <- read.spss("../data/Regression.sav", use.value.labels=TRUE, max.value.labels=Inf, to.data.frame=TRUE)
#Setting the column names to be that used in the dataset
colnames(regression) <- tolower(colnames(regression))
#Get descriptive stastitics by group
#describeBy is part of the psych package so you need to use it
describeBy(regression$normexam,group=regression$girl)
View(regression)
#We are using a .dat file (survey.dat) created from the SPSS file survey.sav  taken from SPSS Survival Manual 6th Edition Julie Pallant
#http://spss.allenandunwin.com.s3-website-ap-southeast-2.amazonaws.com/data-files.html#.Wb0vvnWP-po
#Results on a survey on well being
#We need to load the file so that we can use it in R.
survey <- read.table("../data/survey.dat")
View(survey)
#We are using a .dat file (survey.dat) created from the SPSS file survey.sav  taken from SPSS Survival Manual 6th Edition Julie Pallant
#http://spss.allenandunwin.com.s3-website-ap-southeast-2.amazonaws.com/data-files.html#.Wb0vvnWP-po
#Results on a survey on well being
#We need to load the file so that we can use it in R.
survey <- read.table("../data/survey.dat")
#We are using a .dat file (survey.dat) created from the SPSS file survey.sav  taken from SPSS Survival Manual 6th Edition Julie Pallant
#http://spss.allenandunwin.com.s3-website-ap-southeast-2.amazonaws.com/data-files.html#.Wb0vvnWP-po
#Results on a survey on well being
#We need to load the file so that we can use it in R.
survey <- read.table("../data/survey.dat")
#Setting the column names to be that used in the dataset
colnames(survey) <- tolower(colnames(survey))
library(pastecs) #For creating descriptive statistic summaries
library(ggplot2) #For creating histograms with more detail than plot
library(psych) # Some useful descriptive functions
library(semTools) #For skewness and kurtosis
library(car) # For Levene's test for homogeneity of variance
library(coin)# For Wilcox test (non-parametric)
#Get descriptive stastitics by group
describeBy(survey$tslfest, survey$sex)
#Conduct Levene's test for homogeneity of variance in library car
ltest<-car::leveneTest(tslfest ~ sex, data=survey)
#Pr(F) is your probability
ltest
#Conduct the t-test from package stats
#You can use the var.equal = TRUE option to specify equal variances and a pooled variance estimate
stats::t.test(tslfest~sex,var.equal=TRUE,data=survey)
drinkset<- read.table("../data/Field-BDI-Non-parametric.dat")v
#Get your descriptive statistcs
describeBy(drinkset$bdisun,group=drinkset$drink)
drinkset <- read.table("../data/Field-BDI-Non-parametric.dat")
#Get your descriptive statistcs
describeBy(drinkset$bdisun,group=drinkset$drink)
#Create data subsets for each drink
vodkadata <- subset(drinkset, drink=='Vodka')
beerdata <-subset(drinkset, drink=='Beer')
#Create plots of these
gs <- ggplot(vodkadata, aes(x=vodkadata$bdisun))
gs <- gs + labs(x="Vodka")
gs <- gs + geom_histogram(binwidth=2, colour="black", aes(y=..density.., fill=..count..))
gs <- gs + scale_fill_gradient("Count", low="#DCDCDC", high="#7C7C7C")
gs <- gs + stat_function(fun=dnorm, color="red",args=list(mean=mean(vodkadata$bdisun, na.rm=TRUE), sd=sd(vodkadata$bdisun, na.rm=TRUE)))
gs
gs <- ggplot(beerdata, aes(x=beerdata$bdisun))
gs <- gs + labs(x="Beer")
gs <- gs + geom_histogram(binwidth=2, colour="black", aes(y=..density.., fill=..count..))
gs <- gs + scale_fill_gradient("Count", low="#DCDCDC", high="#7C7C7C")
gs <- gs + stat_function(fun=dnorm, color="red",args=list(mean=mean(beerdata$bdisun, na.rm=TRUE), sd=sd(beerdata$bdisun, na.rm=TRUE)))
gs
# Test for differences on  Sunday
stats::wilcox.test(bdisun~drink, data=drinkset)
# Test for differences on Wednesday
stats::wilcox.test(bdiwed~drink, data=drinkset)
# Test for differences on  Sunday
coin::wilcox_test(bdisun~drink, data=drinkset)
# Test for differences on Wednesday
coin::wilcox_test(bdiwed~drink, data=drinkset)
#Paired T-test:
#t.test(y1,y2,paired=TRUE) # where y1 & y2 are numeric
#Experiment - fear of statistics
edata<-read.table('../data/experim.dat')
setwd("~/Documents/Code/bozicb.github.io/lectures/PSI/Week 5/code")
#Paired T-test:
#t.test(y1,y2,paired=TRUE) # where y1 & y2 are numeric
#Experiment - fear of statistics
edata<-read.table('../data/experim.dat')
# Difference between FearFor our example:
t.test(edata$fost1,edata$fost2,paired=TRUE)
#Non-parametric Repeated measure
#We are using the drink dataset again
#Need to split the file or subset it  and run against each sub-set
vodkadata<-subset(drinkset,drinkset$drink=='Vodka')
beerdata<-subset(drinkset,drinkset$drink=='Beer')
summary(vodkadata)
summary(beerdata)
stats::wilcox.test(vodkadata$bdisun,vodkadata$bdiwed,paired=TRUE)
stats::wilcox.test(beerdata$bdisun,beerdata$bdiwed,paired=TRUE)
#We are using a .dat file (survey.dat) created from the SPSS file survey.sav  taken from SPSS Survival Manual 6th Edition Julie Pallant
#http://spss.allenandunwin.com.s3-website-ap-southeast-2.amazonaws.com/data-files.html#.Wb0vvnWP-po
#Results on a survey on well being
#We need to load the file so that we can use it in R.
survey <- read.table("../data/survey.dat")
#Setting the column names to be that used in the dataset
colnames(survey) <- tolower(colnames(survey))
library(pastecs) #For creating descriptive statistic summaries
library(ggplot2) #For creating histograms with more detail than plot
library(psych) # Some useful descriptive functions
library(semTools) #For skewness and kurtosis
library(car) # For Levene's test for homogeneity of variance
library(coin)# For Wilcox test (non-parametric)
library(pastecs) #For creating descriptive statistic summaries
library(ggplot2) #For creating histograms with more detail than plot
library(psych) # Some useful descriptive functions
library(semTools) #For skewness and kurtosis
library(car) # For Levene's test for homogeneity of variance
library(coin)# For Wilcox test (non-parametric)
#Get descriptive stastitics by group
describeBy(survey$tslfest, survey$sex)
#Conduct Levene's test for homogeneity of variance in library car
ltest<-car::leveneTest(tslfest ~ sex, data=survey)
#Pr(F) is your probability
ltest
#Conduct the t-test from package stats
#You can use the var.equal = TRUE option to specify equal variances and a pooled variance estimate
stats::t.test(tslfest~sex,var.equal=TRUE,data=survey)
drinkset <- read.table("../data/Field-BDI-Non-parametric.dat")
#Get your descriptive statistcs
describeBy(drinkset$bdisun,group=drinkset$drink)
#Create data subsets for each drink
vodkadata <- subset(drinkset, drink=='Vodka')
beerdata <-subset(drinkset, drink=='Beer')
#Create plots of these
gs <- ggplot(vodkadata, aes(x=vodkadata$bdisun))
gs <- gs + labs(x="Vodka")
gs <- gs + geom_histogram(binwidth=2, colour="black", aes(y=..density.., fill=..count..))
gs <- gs + scale_fill_gradient("Count", low="#DCDCDC", high="#7C7C7C")
gs <- gs + stat_function(fun=dnorm, color="red",args=list(mean=mean(vodkadata$bdisun, na.rm=TRUE), sd=sd(vodkadata$bdisun, na.rm=TRUE)))
gs
gs <- ggplot(beerdata, aes(x=beerdata$bdisun))
gs <- gs + labs(x="Beer")
gs <- gs + geom_histogram(binwidth=2, colour="black", aes(y=..density.., fill=..count..))
gs <- gs + scale_fill_gradient("Count", low="#DCDCDC", high="#7C7C7C")
gs <- gs + stat_function(fun=dnorm, color="red",args=list(mean=mean(beerdata$bdisun, na.rm=TRUE), sd=sd(beerdata$bdisun, na.rm=TRUE)))
gs
# Test for differences on  Sunday
stats::wilcox.test(bdisun~drink, data=drinkset)
# Test for differences on Wednesday
stats::wilcox.test(bdiwed~drink, data=drinkset)
# Test for differences on  Sunday
coin::wilcox_test(bdisun~drink, data=drinkset)
# Test for differences on Wednesday
coin::wilcox_test(bdiwed~drink, data=drinkset)
View(survey)
setwd("~/Documents/Code/bozicb.github.io/lectures/WWD/labs/Week 6")
# Question 1
#Import the file places.csv and determine the place names that end in either a ‘y’ or a ‘u’
dates <- read.csv('dates.csv')
View(dates)
# Question 1
#Import the file places.csv and determine the place names that end in either a ‘y’ or a ‘u’
dates <- read.csv('places.csv')
# Question 1
#Import the file places.csv and determine the place names that end in either a ‘y’ or a ‘u’
places <- read.csv('places.csv')
View(places)
grep('y$',places$city)
grep('[y,u]$',places$city)
places$cities[grep('[y,u]$',places$city)]
lapply(places$city,grep('[y,u]$',places$city))
lapply(places$city,function(x) grep('[y,u]$',x))
lapply(places$city, function(x) grep('[y,u]$',x),value=T)
lapply(places$city, function(x) grep('[y,u]$',x,value=T))
?grep
lapply(places$city, function(x) grepl('[y,u]$',x))
places$cities[lapply(places$city, function(x) grepl('[y,u]$',x))]
lapply(places$city, function(x) sub('[y,u]$',x))
lapply(places$city, function(x) regexpr('[y,u]$',x))
lapply(places$city, function(x) regexec('[y,u]$',x))
lapply(places$city, function(x) grep('[y,u]$',x,value=T))
lapply(unlist(places$city), function(x) grep('[y,u]$',x,value=T))
typeof(places$cities)
typeof(places$city)
places$city
lapply(places$city, function(x) grep('[y,u]$',x,value=T))
places$city
places$cities[grep('[y,u]$',places$city,value=T)]
lapply(places$city, function(x) grep('[y,u]$',x,value=T,perl=T))
typeof(places$city)
typeof(places$city_ascii)
lapply(places$city_ascii, function(x) grep('[y,u]$',x,value=T))
typeof(places$city_ascii)
typeof(places$city)
View(places)
lapply(places$city_ascii, function(x) unlist(grep('[y,u]$',x,value=T)))
unlist(lapply(places$city_ascii, function(x) grep('[y,u]$',x,value=T)))
?unlist
str_dect(s,"$^$")
# Question 2
# How would you match the literal string “$^$”?
library(stringr)
s <- "$^$"
str_dect(s,"$^$")
str_detect(s,"$^$")
?str_detect
str_detect(s,"\$\^\$")
str_detect(s,"\$")
str_detect(s,"\$^$")
str_detect(s,'\$')
grepl(s,'\$')
grepl(s,"\$")
grepl(s,"\[$^$]")
grepl(s,"[$^$]")
grepl("[$^$]",s)
grepl("\$\^\$",s)
str_detect("\$\^\$",s)
str_detect("\\$\\^\\$",s)
str_detect(s,"\\$\\^\\$")
# Question 3
#Given the vector of common words in stringr::words, create regular expressions to find all the words that:
#  a. Start with “y”
#  b. Are exactly 3 letters long
#  c. Have seven letters or more
stringr::words
# Question 3
#Given the vector of common words in stringr::words, create regular expressions to find all the words that:
#  a. Start with “y”
#  b. Are exactly 3 letters long
#  c. Have seven letters or more
str_detect(stringr::words,"$")
lapply(stringr::words, function(x) grep('$y',x,value=T))
unlist(lapply(stringr::words, function(x) grep('$y',x,value=T)))
unlist(lapply(stringr::words, function(x) grep('$[y]',x,value=T)))
unlist(lapply(stringr::words, function(x) grep('^[y]',x,value=T)))
unlist(lapply(stringr::words, function(x) grep('^y',x,value=T)))
unlist(lapply(stringr::words, function(x) grep('[a-z]{3}',x,value=T)))
unlist(lapply(stringr::words, function(x) grep('a-z{3}',x,value=T)))
unlist(lapply(stringr::words, function(x) grep('^[a-z]{3}$',x,value=T)))
unlist(lapply(stringr::words, function(x) grep('^[a-z]{7+}$',x,value=T)))
unlist(lapply(stringr::words, function(x) grep('^[a-z]{7,}$',x,value=T)))
# Question 4
# Import the file contained in isoc_ec_ibuy.tsv.gz and use string processing to separate the columns.
eu_data <- read.table("isoc_ec_ibuy.tsv",sep = '\t',header = TRUE)
View(eu_data)
View(eu_data)
library(dplyr)
separate()
?separate
??separate
library(dplyr)
?separate
?str_split_fixed
library(tidyr)
?separate
separate(eu_data,indic_is.ind_type.unit.geo.time,c('indic_is','ind_type','unit','geo','time'),',')
eudata <- separate(eu_data,indic_is.ind_type.unit.geo.time,c('indic_is','ind_type','unit','geo','time'),',')
# Question 5
# Your colleague, who does data modelling, is looking for a quick assessment of the usability of the data in this data frame (Question 4). Can you help her out with some summary information.
summary(eudata$indic_is)
summary(eudata$ind_type)
summary(eudata$unit)
summary(eudata$geo)
summary(eudata$time)
View(eudata)
summary(eudata$X2017)
describe(eudata$X2017)
describe(eudata)
View(eudata)
summarise(eudata)
summarise(eudata)
summarise(eudata$indic_is)
# Question 5
# Your colleague, who does data modelling, is looking for a quick assessment of the usability of the data in this data frame (Question 4). Can you help her out with some summary information.
summarise(eudata$geo,total=sum(geo),mode=mode(geo))
# Question 5
# Your colleague, who does data modelling, is looking for a quick assessment of the usability of the data in this data frame (Question 4). Can you help her out with some summary information.
summarise(eudata,total=sum(geo),mode=mode(geo))
# Question 5
# Your colleague, who does data modelling, is looking for a quick assessment of the usability of the data in this data frame (Question 4). Can you help her out with some summary information.
summarise(eudata,mode=mode(geo))
# Question 5
# Your colleague, who does data modelling, is looking for a quick assessment of the usability of the data in this data frame (Question 4). Can you help her out with some summary information.
summarise(eudata,mode=mode(unlist(geo))
# Question 5
# Your colleague, who does data modelling, is looking for a quick assessment of the usability of the data in this data frame (Question 4). Can you help her out with some summary information.
summarise(eudata,mode=mode(unlist(geo))
)# Question 6
# Question 5
# Your colleague, who does data modelling, is looking for a quick assessment of the usability of the data in this data frame (Question 4). Can you help her out with some summary information.
summarise(eudata,mode=mode(unlist(geo)))
# Question 5
# Your colleague, who does data modelling, is looking for a quick assessment of the usability of the data in this data frame (Question 4). Can you help her out with some summary information.
summarise(eudata,mode=mode(unlist(eudata$geo)))
# Question 5
# Your colleague, who does data modelling, is looking for a quick assessment of the usability of the data in this data frame (Question 4). Can you help her out with some summary information.
summarise(eudata,mode=mode((eudata$geo)))
# Question 5
# Your colleague, who does data modelling, is looking for a quick assessment of the usability of the data in this data frame (Question 4). Can you help her out with some summary information.
summary(eudata)
summary(eudata$goe)
summary(eudata$geo)
typeof(eudata)
eudata
# Question 6
# Use the appropriate lubridate function to parse each of the following dates: “January 1, 2010”
# “2015-Mar-07”
# “06-Jun-2017”
# c(“August 19 (2015)”, “July 1 (2015)”) “12/30/14” # Dec 30, 2014
library(lubridate)
ymd("2015-Mar-07")
dmy("06-Jun-2017")
mdy("January 1, 2010")
mdy("Dec 30, 2014")
mdy(c("August 19 (2015)", "July 1 (2015)"))
# Question 7
# Using the flights dataset in the nycflights13 library, confirm Hadley’s hunch that the early departures of flights in minutes 20 – 30 and 50 – 60 are caused by scheduled flights that leave early.
library(nycflights13)
flights_dt %>%
mutate(
minute = minute(dep_time),
early = dep_delay < 0
) %>%
group_by(minute) %>%
summarise(
early = mean(early, na.rm = TRUE),
n = n()
) %>%
ggplot(aes(minute, early)) +
geom_line()
flights_dt <- flights %>%
filter(!is.na(dep_time), !is.na(arr_time)) %>%
mutate(
dep_time = make_datetime_100(year, month, day, dep_time),
arr_time = make_datetime_100(year, month, day, arr_time),
sched_dep_time = make_datetime_100(year, month, day, sched_dep_time),
sched_arr_time = make_datetime_100(year, month, day, sched_arr_time)
)
make_datetime_100 <- function(year, month, day, time) {
make_datetime(year, month, day, time %/% 100, time %% 100)
}
flights_dt <- flights %>%
filter(!is.na(dep_time), !is.na(arr_time)) %>%
mutate(
dep_time = make_datetime_100(year, month, day, dep_time),
arr_time = make_datetime_100(year, month, day, arr_time),
sched_dep_time = make_datetime_100(year, month, day, sched_dep_time),
sched_arr_time = make_datetime_100(year, month, day, sched_arr_time)
)
flights_dt %>%
mutate(
minute = minute(dep_time),
early = dep_delay < 0
) %>%
group_by(minute) %>%
summarise(
early = mean(early, na.rm = TRUE),
n = n()
) %>%
ggplot(aes(minute, early)) +
geom_line()
# Question 8
# The file ‘dates.csv’ contains data about the dates of this year. Suppose a friend of yours who gets paid on the first Friday of each month is curious to know these dates in 2018. Can you find the answer for them?
dates <- read.csv('dates.csv')
View(dates)
dates[wday(dates,label = TRUE) == "Fri" & day(dates) <= 7]
x
dates[wday(dates,label = TRUE) == "Fri" & day(dates) <= 7]
dates$dates[wday(dates$dates,label = TRUE) == "Fri" & day(dates$dates) <= 7]
View(dates)
setwd("~/Documents/Code/bozicb.github.io/lectures/PSI/Week 5/data")
edata <- read.table('experim.dat')
edata <- read.table('experim.dat')
t.test(edata$fost1, edata$fost2, paired=T)
View(edata)
drinkset <- read.table('Field-BDI-Non-parametric.dat')
vodkadata <- subset(drinkset, drinkset$drink == 'Vodka')
beerdata <-subset(drinkset, drinkset$drink == 'Beer')
