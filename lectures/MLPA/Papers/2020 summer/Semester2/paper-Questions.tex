% Filename  : samplepaper.tex
% Purpose   : A sample exam paper to demonstrate how to use the 'ditpaper'
%             TeX class.
% Author    : Emmet Caulfield
% Revision  : $Id: samplepaper.tex 2 2006-02-19 20:34:45Z emmet $
% Repository: $HeadURL: http://svn.netrogen.lan/tex-ditpaper/trunk/samplepaper.tex $
%

% 'nosolution' (default) and 'solution' toggle the inclusion of solutions
% in the output. The tag nosolution, below, is replaced by 'sed'
% in the Makefile to cause both the paper and the solutions to be produced.
%\documentclass[nosolution]{ditpaper}

\documentclass[nosolution]{ditpaper}
%\documentclass[solution]{ditpaper}
%\documentclass[nosolution]{ditpaper}

%\usepackage{epsf}
\usepackage{fleqn}
%\usepackage{rotating}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{eurosym}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Start newcommand defs taken from aima slides style file %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\def\mysum{\begin{Huge}\mbox{$\Sigma$}\end{Huge}}
\def\myint{\begin{LARGE}\mbox{$\int$}\end{LARGE}}
\def\myprod{\begin{Huge}\mbox{$\Pi$}\end{Huge}}

\newcommand{\smbf}[1]{\mbox{{\smathbold #1}}}
\newcommand{\mbf}[1]{\mbox{{\bf #1}}}

\newcommand{\sr}[1]{\mathrel{\raisebox{-0.6ex}{$\stackrel{#1}{\longrightarrow}$}}}
\newcommand{\srbox}[1]{\sr{\fboxsep=1pt\fbox{$\,{\scriptstyle #1}\,$}}}
\newcommand{\srboxbox}[1]{\sr{\fboxsep=1pt\fbox{\fbox{$\,{\scriptstyle #1}\,$}}}}
\newcommand{\keyword}[1]{{\textbf{#1}}\index{#1}}

\newcommand{\featN}[1]{\textsc{#1}}
\newcommand{\featL}[1]{\textit{`#1'}}

\def\Diff{\mbox{{\it Diff}}}

%%%%%% probability and decision theory
\newcommand{\pv}{\mbf{P}}
\newcommand{\qv}{\mbf{Q}}
\newcommand{\given}{\mid}
\def\transition#1#2{q(#1\rightarrow #2)}
\newcommand{\otherthan}{\overline}
\newcommand{\Parents}{Parents}
\newcommand{\parents}{parents}
\newcommand{\Children}{Children}
\newcommand{\children}{children}
\newcommand{\MarkovBlanket}{MB}
\newcommand{\markovBlanket}{mb}

\def\X{\mbf{X}}
\def\x{\mbf{x}}
\def\sx{\smbf{x}}
\def\Y{\mbf{Y}}
\def\y{\mbf{y}}
\def\sy{\smbf{y}}
\def\E{\mbf{E}}
\def\e{\mbf{e}}
\def\D{\mbf{D}}
\def\d{\mbf{d}}
\def\sbe{\smbf{e}}
\def\sE{\smbf{E}}
\def\T{\mbf{T}}
\def\O{\mbf{O}}
\def\se{\smbf{e}}
\def\Z{\mbf{Z}}
\def\z{\mbf{z}}
\def\sz{\smbf{z}}
\def\F{\mbf{F}}
\def\f{\mbf{f}}
\def\A{\mbf{A}}
\def\B{\mbf{B}}
\def\C{\mbf{C}}
\def\b{\mbf{b}}
\def\m{\mbf{m}}
\def\I{\mbf{I}}
\def\H{\mbf{H}}
\def\zeroes{\mbf{0}}
\def\ones{\mbf{1}}
\def\ev{\mbf{ev}}
\def\fv{\mbf{ev}}
\def\sv{\mbf{sv}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% End newcommand defs taken from aima slides style file %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





% These must be set or bizarre defaults will be used:
\facility{Kevin Street, Dublin 8}
\course{BSc. (Honours)\\Degree in Information Systems /\\ Information Technology\\(Part-time)}
\examcode{DT249, DT255}
\stage{Stage 4}
\session{Summer Examinations 2016/2017}
\title{Artificial Intelligence II [CMPU4011]}
\examiners{Mr. Giancarlo Salton\\
Dr. Deirdre Lillis\\
Dr. Rem Collier}
\examdate{Date \& Time TBA.}
\examtime{Duration: 2 Hours}
\instructions{Question 1 is \textbf{compulsory}\par{} Answer Question 1 (40 marks) \textbf{and}\par{} any 2 Other Questions (30 marks each).}

\begin{document}

%aima chapters 18
% inductive bias, learning theory - supervised/unsupervised, overfitting, lazy/eager learner, classification v regression, false positive v false negatives, linear separability, consistency, evaluation

\question
\begin{enumerate}

	% machine learning as an ill-posed problem
	\item Why is machine learning an \textbf{ill-posed problem}?
	\marks{5}
	\begin{answer}
		Machine learning is an \keyword{ill-posed problem} for two reasons: first, when dealing with large datasets, it is likely there will be \textbf{noise} in the data and models consistent with a noisy dataset will make incorrect predictions; and second, for the majority of machine learning problems, the training set represents only a small sample of possible set of instances in the domain.
	\end{answer}

	% inductive bias
	\item What is the \textbf{inductive bias} of a machine learning algorithm?
	\marks{5}
	\begin{answer}
		The \keyword{inductive bias} is the set of assumptions that define the model selection criteria of a machine learning algorithm. There are two types of inductive bias that a machine learning algorithm can use: the \keyword{restriction bias} constrains the set of models the algorithm will consider during the learning process; and the \keyword{preference bias} guides the learning algorithm to prefer certain models over the others.
		\end{answer}

	% using the wrong inductive bias
	\item Explain what can go wrong when a machine learning classifier uses the wrong \textbf{inductive bias}.
	\marks{5}
	\begin{answer}
		\begin{itemize}
			\item If the inductive bias of the learning algorithm constrains the search to only consider simple hypotheses we may have excluded the real function from the hypothesis space. In other words, the true function is \textbf{unrealizable} in the chosen hypothesis space, (i.e., we are \textbf{underfitting}).
			\item If the inductive bias of the learning algorithm allows the search to consider complex hypotheses, the model may hone in on irrelevant factors in the training set. In other words the model with \textbf{overfit} the training data.
		\end{itemize}
	\end{answer}

	% confusion matrix, accuracy, precision, recall and F1
	\item Table \ref{tab:testSetPredictions2}, on the next page, shows the predictions made for a categorical target feature by a model for a test dataset. Based on this test set, calculate the evaluation measures listed below.

	\begin{enumerate}

		% confusion matrix
		\item A \textbf{confusion matrix}
		\marks{6}
		\begin{answer}
			The confusion matrix can be written as
			\vspace{1em}
			\noindent\begin{scriptsize}
			\begin{tabular}{c c c  c }
    		& &  \multicolumn{2}{c}{Prediction} \\
  			& &  \featL{true} &  \featL{false} \\
  			\hline
  			\multirow{2}{*}{\parbox{1.1cm}{\raggedleft Target}}  & \featL{true} & $8$	&	$1$ \\
  			& \featL{false} & $0$	&	$11$
			\end{tabular}
			\end{scriptsize}
			\vspace{1em}
		\end{answer}

		% classification accuracy
		\item The \textbf{classification accuracy}
		\marks{4}
		\begin{equation*}
			classification~accuracy= \frac{\left(TP + TN\right)}{\left(TP + TN + FP + FN\right)}
		\end{equation*}
		\begin{answer}
			Classification accuracy can be calculated as
			\begin{alignat*}{2}
			misclassification~rate & = \frac{\left(TP + TN\right)}{\left(TP + TN + FP + FN\right)} \\
			 & = \frac{\left(8 + 11\right)}{\left(8 + 11 + 0 + 1\right)} \\
			 & = 0.95
			\end{alignat*}
		\end{answer}

		% precision, recall and F1
		\item The \textbf{precision}, \textbf{recall}, and \textbf{F$_1$ measure}
		\marks{15}
		\begin{alignat*}{2}
		precision & = \frac{TP}{\left(TP + FP\right)} \\
		recall & = \frac{TP}{\left(TP + FN\right)} \\
		F_1~measure & = 2 \times \frac{\left(precision \times recall \right)}{\left(precision + recall\right)} \\
		\end{alignat*}
		\begin{answer}
			We can calculate precision and recall as follows (assuming that the \featL{true} target level is the positive level):
			\begin{alignat*}{2}
				precision & = \frac{TP}{\left(TP + FP\right)} \\
				& = \frac{8}{\left(8 + 0\right)} \\
				& = 1.000 \\
				recall & = \frac{TP}{\left(TP + FN\right)} \\
				& = \frac{8}{\left(8 + 1\right)} \\
				& = 0.889
			\end{alignat*}
			Using these figures, we can calculate the F$_1$ measure as
			\begin{alignat*}{2}
				F_1~measure & = 2 \times \frac{\left(precision \times recall \right)}{\left(precision + recall\right)} \\
				& = 2 \times \frac{\left(1.000 \times 0.889 \right)}{\left(1.000 + 0.889\right)} \\
				& = 0.941
			\end{alignat*}
		\end{answer}

	\end{enumerate}
\end{enumerate}

\begin{table}[htb]
	\caption{The predictions made by a model for a categorical target on a test set of 20 instances }
	\label{tab:testSetPredictions2}
	\begin{center}
		{\setlength{\tabcolsep}{2em}
		\begin{tabular}{cc}
			\hline
			\begin{minipage}{0.3\textwidth}
				\raggedright
					{\setlength{\tabcolsep}{1em}
				\begin{tabular}{@{\extracolsep{\fill}}c c c@{}}
					ID	 & Target & Prediction \\
					\hline
					1	&	false	&	false \\
					2	&	true	&	true \\
					3	&	false	&	false \\
					4	&	true	&	true \\
					5	&	false	&	false \\
					6	&	false	&	false \\
					7	&	true	&	false \\
					8	&	true	&	true \\
					9	&	true	&	true \\
					10	&	true	&	true \\
					\hline
				\end{tabular}
				}
			\end{minipage}
			&
			\begin{minipage}{0.3\textwidth}
				\raggedleft
					{\setlength{\tabcolsep}{1em}
				\begin{tabular}{@{\extracolsep{\fill}}c c c@{}}
				ID	 & Target	& Prediction \\
				\hline
				11	&	false	&	false \\
				12	&	false	&	false \\
				13	&	false	&	false \\
				14	&	false	&	false \\
				15	&	true	&	true \\
				16	&	false	&	false \\
				17	&	true	&	true \\
				18	&	true	&	true \\
				19	&	false	&	false \\
				20	&	false	&	false \\
				\hline
				\end{tabular}
				}
			\end{minipage}\\
		\end{tabular}
		}
	\end{center}
\end{table}

\clearpage

%Q2
% knn and CBR
% information theory, entropy, Decision Trees, Inductive logic programming
\newpage

\question

\begin{enumerate}

	% k-NN question
	\item Table \ref{tab:3nn-data}, on the next page, lists a dataset containing examples described by two descriptive features, \textbf{Feature 1} and \textbf{Feature 2}, and labelled with a target class \textbf{Target}. Table \ref{tab:3nn-query}, also on the next page, lists the details of a query for which we want to predict the target label. We have decided to use a \textbf{3-Nearest Neighbor} model for this prediction and we will use Euclidean distance as our distance metric:
		\begin{center}
		$d(x_1,x_2)=\sqrt{\sum_{i=1}^{n} \left(\left(x_1.f_i - x_2.f_i \right)^2 \right)}$
		\end{center}

	\begin{enumerate}

			% first prediction
			\item With which target class (\textbf{TypeA}  or \textbf{TypeB}) will our \textbf{3-Nearest Neighbor} model label the query? Provide an explanation for your answer.
			\marks{8}
			\begin{answer}
				The first stage is to calculate the Euclidean distance between each of the examples and the query:
				\begin{center}
					\begin{tabular}{|c|c|}
					ID & Euclidean Distance \\
					\hline
					101 & 60000\\
					102 & 120000\\
					103 & 120000\\
					104 & 180000\\
					105 & 240000\\
					\hline
					\end{tabular}
				\end{center}
				 From this table we can see that the three closest examples to the query are examples 101, 102, and 103. Example 101 has a target label of TypeA and both 102 and 103 have target labels TypeB. Consequently TypeB is the majority label in local model constructed by the 3-Nearest Neighbor classifier for this query instance and the query will be labelled with class TypeB.
			\end{answer}

		% range normalization
		\item There is a large variation in range between \textbf{Feature 1} and \textbf{Feature 2}. To account for this we decide to normalize the data. Compute the normalized versions of Feature 1 and Feature 2  to four decimal places of precision using range normalization
				\begin{center}
				$x_i.f^\prime=\frac{x_i.f - min(f)}{max(f)-min(f)}$
				\end{center}
		\marks{4}
		\begin{answer}
			\begin{center}
				\begin{tabular}{cccc}
				\hline
					ID & Feature 1 & Feature 2  & Target \\
					\hline
					101 &	0.1667 &	0.2  &	TypeA\\
					102 &	0.0000 & 0	 &	TypeB\\
					103 &	0.6667 & 0.8 &	TypeB\\
					104 &	0.8333 &	0.4 &	TypeA\\
					105 &	1.0000 & 1 &	TypeB\\
				\hline
				\end{tabular}
			\end{center}
		\end{answer}

		% new prediction with normalized data
		\item Assuming we use the normalized dataset as input, with which target class (\textbf{TypeA}  or \textbf{TypeB}) will our \textbf{3-Nearest Neighbor} model label the query? Provide an explanation for your answer. \marks{8}
			\begin{answer}
				The normalize query instance is:
				\begin{center}
				\begin{tabular}{|c|c|c|c|}
				\hline
				ID & Feature 1 & Feature
				2  & Target \\
				\hline
				250 & 0.2 & 0.3333 & ?\\
				\hline
				\end{tabular}
				\end{center}
				The Euclidean distances between the normalized data and normalized query are:
				\begin{center}
					\begin{tabular}{|c|c|}
					ID & Euclidean Distance \\
					\hline
					101	& 0.1667\\
					102	& 0.3887\\
					103	& 0.6864\\
					104	& 0.5385\\
					105	& 1.0414\\
					\hline
					\end{tabular}
				\end{center}
				From this table we can see that the 3 closest neighbors are: 101, 102 and 104. 101 and 104 are both labelled as class \textbf{TypeA}. So \textbf{TypeA} is the majority class in the neighborhood and the query will be labelled as belonging to it.
			\end{answer}
	\end{enumerate}

	\item A dataset showing the decisions made by a professional basketball team on whether to draft college players based on 4 features (1 continuous and 3 categorical) as listed in Table \ref{tab:players} on the next page.  (Note that Table \ref{tab:info-eqs}, also on the next page, lists some equations that you may find useful for this question.)
	\begin{enumerate}

		% entropy calculations
		\item Given that the \textsc{Draft} column lists the values of the target variable, compute the entropy for this dataset.
		\marks{5}
		\begin{answer}
				There are 6 positive and 6 negative examples in this dataset. This means that the entropy for the dataset is:
				\begin{eqnarray*}
					I(\frac{6}{12}, \frac{6}{12}) &=& -\frac{6}{12}\log_2\frac{6}{12} + -\frac{6}{12}\log_2 \frac{6}{12}\\
								~ &=& (-\frac{1}{2}\log_2\frac{1}{2}) + (-\frac{1}{2}\log_2\frac{1}{2}) \\
					~ &=&  -\frac{1}{2}(-1) + -\frac{1}{2}(-1) \\
					 ~ &=&  1bit
				\end{eqnarray*}
		\end{answer}

		% what entropy tells us about the dataset
		\item What the entropy tells us about a dataset? What happens with the distribution of a dataset if the entropy is increased?
		\marks{5}
		\begin{answer}
			\begin{enumerate}
				\item The entropy is a measure of impurity of the elements in a dataset. It can be thought as the uncertainty associated with guessing the result if we are to make a random selection from the set.
				\item If the entropy of the dataset is increased, the dataset becomes less homogeneous. Therefore, the uncertainty associated with the dataset if we are to make a random selection is also increased.
			\end{enumerate}
		\end{answer}
	\end{enumerate}

\end{enumerate}

\newpage

\begin{table}[htp]
	\caption{Dataset for the 3-Nearest Neighbor question}
	\begin{center}
		\begin{tabular}{cccc}
			\hline
			ID & Feature 1 & Feature 2  & Target \\
			\hline
			101 & 180000 &	4  & TypeA\\
			102 & 120000 &	3  & TypeB\\
			103 & 360000 &	7  & TypeB\\
			104 & 420000 &	5  & TypeA\\
			105 & 480000 &	8  & TypeB\\
			\hline
		\end{tabular}
	\end{center}
	\label{tab:3nn-data}
\end{table}%

\begin{table}[htp]
	\caption{Query instance for the 3-Nearest Neighbor question.}
	\begin{center}
		\begin{tabular}{cccc}
			\hline
			ID & Feature 1 & Feature 2  & Target \\
			\hline
			250 & 240000 &	4 & ?\\
			\hline
		\end{tabular}
	\end{center}
	\label{tab:3nn-query}
\end{table}%


\begin{table}[h]
	%\begin{tiny}
	\caption{A dataset showing the decisions made by a professional basketball team on whether to draft college players.}
	\begin{center}
		\begin{tabular}{lcccccccc}
			\hline
			%\textbf{Example} & \multicolumn{5}{c||}{\textbf{Attributes}} & \textbf{Target} \\
			%\cline{2-6}
			\textsc{ID} & \textsc{Age} & \textsc{Speed} & \textsc{Agility} & \textsc{Ability} & \textsc{Draft} \\
			\hline
			$1$  & 20 & 1 & 1 & 3 &  $F$\\
			$2$    & 21 & 2 & 2 & 1 &  $F$\\
			$3$    & 20 & 2 & 1 & 2 &  $F$\\
			$4$    & 22 & 2 & 1  & 1  &  $F$\\
			$5$    & 22 & 4 & 4  & 4 &  $T$\\
			$6$    & 21 & 5 & 4  & 5 &  $T$\\
			$7$    & 23 & 5 & 5  & 4 &  $T$\\
			$8$    & 19 & 4 & 5  & 5 &  $T$\\
			$9$     & 22 & 5 & 5  & 5 &  $T$\\
			$10$ & 21 & 1 & 1  & 2 &  $F$\\
			$11$ & 20 & 5 & 5  & 4 &  $T$\\
			$12$ & 21 & 3 & 1  & 1 &  $F$\\
			\hline
		\end{tabular}
	\end{center}
	%\end{tiny}
	\label{tab:players}

\end{table}

\begin{table}[!hb]
		\renewcommand{\arraystretch}{2}
	\begin{center}
		\caption{Equations for entropy.}
		\label{tab:info-eqs}
		\begin{tabular}{ll}
			\hline
			$H(\mathbf{f}, \mathcal{D})$ & $= -\displaystyle\sum_{l \in levels(f)} P(f=l) \times log_2(P(f=l))$\\
			\hline
		\end{tabular}
	\end{center}
\end{table}

\newpage

\question Table \ref{tab:rest} lists a dataset of books and whether or not they were purchased by an individual (i.e., the feature \textsc{Purchased} is the target feature in this domain).

\begin{enumerate}

	\item Calculate the probabilities (to four places of decimal) that a \textbf{naive Bayes} classifier would use to represent this domain.
	\marks{18}
	\begin{answer}
		A naive Bayes classifier would require the prior probability for each level of the target feature and the conditional probability for each level of each descriptive feature given each level of the target feature:
		\begin{footnotesize}
		\begin{tabular}{ll}
		$P(Purchased=Yes)=0.4$ & $P(Purchased=No)=0.6$\\
		$P(2nd Hand=True|Purchased=Yes)=0.5 $& $P(2nd Hand=True|Purchased=No)=0.5$\\
		$P(2nd Hand=False|Purchased=Yes)=0.5 $& $P(2nd Hand=False|Purchased=No)=0.5$\\
		$P(Genre=Literature\mid Purchased=Yes)= 0.25$ & $P(Genre=Literature\mid Purchased=No)=0.1667$\\
		$P(Genre=Romance\mid Purchased=Yes)= 0.5$ & $P(Genre=Romance\mid Purchased=No)=0.3333$\\
		$P(Genre=Science\mid Purchased=Yes)= 0.25$ & $P(Genre=Science\mid Purchased=No)=0.5$\\
		$P(Price=Cheap\mid Purchased=Yes)= 0.5$ & $P(Price=Cheap\mid Purchased=No)=0.5$\\
		$P(Price=Reasonable\mid Purchased=Yes)= 0.25$ & $P(Price=Reasonable\mid Purchased=No)=0.3333$\\
		$P(Price=Expensive\mid Purchased=Yes)= 0.25$ & $P(Price=Expensive\mid Purchased=No)=0.1667$\\
		\end{tabular}
		\end{footnotesize}
	\end{answer}

	%
	\item Assuming conditional independence between features given the target feature value, calculate the \textbf{probability} of each outcome (\textsc{Purchased}=Yes, and \textsc{Purchased}=No) for the following book (marks will be deducted if workings are not shown, round your results to four places of decimal)\\
	\begin{center}
		\textsc{2nd Hand}=False, \textsc{Genre}=Literature, \textsc{Cost}=Expensive
	\end{center}
	\marks{10}
	\begin{answer}
		The initial score for each outcome is calculated as follows:\\
		$(Purchased=Yes) =  0.5 \times 0.25 \times 0.25 \times 0.4 = 0.0125$\\
		$(Purchased=No) =  0.5 \times 0.1667 \times 0.1667 \times 0.6 = 0.0083$\\
		However, these scores are not probabilities. To get real probabilities we must normalise these scores. The normalisation constant is calculated as follows:\\
		$\alpha=0.0125+0.0083=0.0208$\\
		The actual probabilities of each outcome is then calculated as:
		$P(Purchased=Yes) =  \frac{0.0125}{0.0208}=(0.600961...)=0.6010$ \\
		$P(Purchased=No) =  \frac{0.0083}{0.0208}=(0.399038...)=0.3990$\\
	\end{answer}

	\item What prediction would a \textbf{naive Bayes} classifier return for the 	above book?			\marks{2}
	\begin{answer}
		A naive Bayes classifier returns outcome with the maximum a posteriori probability as its prediction. In this instance the outcome \textsc{Purchased}=Yes is the MAP prediction and will be the outcome returned by a naive Bayes model.
	\end{answer}

\end{enumerate}

\begin{table}[h]
	%\begin{tiny}
	\begin{center}
		\caption{A dataset describing the a set of books and whether or not they were purchased by an individual.}
		\label{tab:rest}
		\vspace{0.5em}
		\begin{tabular}{lcccc}
			\hline
			 ID & \textsc{2nd Hand} & \textsc{Genre} & \textsc{Cost} & \textsc{Purchased} \\
			\hline
			$1$  & False & Romance & Expensive &  Yes\\
			$3$    & True & Romance & Cheap &   Yes\\
			$4$    & False & Science & Cheap &   Yes\\
			$10$ & True & Literature & Reasonable &   Yes\\
			$2$    & False & Science & Cheap &   No\\
			$5$    & False & Science & Expensive &   No\\
			$6$    & True & Romance & Reasonable &   No\\
			$7$    & True & Literature & Cheap &   No\\
			$8$    & False & Romance & Reasonable &   No\\
			$9$     & True & Science & Cheap &   No\\
			\hline
		\end{tabular}
	\end{center}
	%\end{tiny}
\end{table}
%
\newpage
\clearpage

%Q4
%Linear Regression Neural Nets, SVMs, Ensemble Learning

\question
\begin{enumerate}

	% logistic regression
	\item A multivariate linear regression model has been built to predict the \textsc{heating load} in a residential building based on a set of descriptive features describing the characteristics of the building. Heating load is the amount of heat energy required to keep a building at a specified temperature, usually $65^{\circ}$ Fahrenheit, during the winter regardless of outside temperature. The descriptive features used are the overall surface area of the building, the height of the building, the area of the building's roof, and the percentage of wall area in the building that is glazed. This kind of model would be useful to architects or engineers when designing a new building. The trained model is
	\begin{alignat*}{2}
		\textsc{Heating~Load} = & -26.030 + 0.0497 \times \textsc{Surface~Area} \\
		& + 4.942\times \textsc{Height} - 0.090 \times \textsc{Roof~Area} \\
		& + 20.523 \times \textsc{Glazing~Area}
	\end{alignat*}
	Use this model to make predictions for each of the query instances shown in the Table \ref{tab:heating} on the next page.
	\marks{12}
	\begin{answer}
		Calculating the predictions made by the model simply involves inserting the descriptive features from each query instance into the prediction model.
		\begin{description}
			\item[1:] $-26.030 + 0.0497 \times 784.0 + 4.942\times 3.5 - 0.090 \times 220.5 + 20.523 \times 0.25$ \\
			$= 15.5$
			\item[2:] $-26.030+0.0497\times710.5+4.942\times3.0-0.09\times210.5+20.523\times0.10$ \\
			$=7.2$
		\end{description}
	\end{answer}

	\item A multivariate logistic regression model has been built to predict the propensity of shoppers to perform a repeat purchase of a free gift that they are given. The descriptive features used by the model are the age of the customer, the average amount of money the customer spends on each visit to the shop, and the average number of visits the customer makes to the shop per week. This model is being used by the marketing department to determine who should be given the free gift. The trained model is
	\begin{alignat*}{2}
		\textsc{Repeat~Purchase} = & -3.82398 - 0.02990   \times \textsc{Age} \\
		& + 0.74572 \times \textsc{Shop~Frequency}\\
		& + 0.02999 \times \textsc{Shop~Value}
	\end{alignat*}
	And, the logistic function is defined as:
	\begin{equation*}
		logistic(x)=\frac{1}{1+e^{-x}}
	\end{equation*}
	Assuming that the \textit{yes} level is the positive level and the classification threshold is $0.5$, use this model to make predictions for each of the query instances shown in Table \ref{tab:logregress} on the next page.
	\marks{18}
	\begin{answer}
	Calculating the predictions made by the model simply involves inserting the descriptive features from each query instance into the prediction model. With this information, the predictions can be made as follows:
	\begin{description}
		\item[1:] $Logistic(-3.82398+-0.0299\times56+0.74572\times1.6+0.02999\times109.32)$ \\
		$=Logistic(-1.02672) =\frac{1}{1-e^{1.02672}}$\\
		$=0.26372 \Rightarrow \textit{no}$
		\item[2:] $Logistic(-3.82398+-0.0299\times21+0.74572\times4.92+0.02999\times11.28)$ \\
		$=Logistic(-0.44465)  =\frac{1}{1-e^{0.44465}}$ \\
		$=0.390633 \Rightarrow \textit{no}$
		\item[3:] $Logistic(-3.82398+-0.0299\times48+0.74572\times1.21+0.02999\times161.19)$ \\
		$=Logistic(0.477229)   =\frac{1}{1-e^{-0.477229}}$ \\
		$=0.6205 \Rightarrow \textit{yes}$
	\end{description}
	\end{answer}
\end{enumerate}


\newpage

\begin{table}[!hb]
	\caption{The queries for the multivariate linear regression \textsc{heating load} question}
	\label{tab:heating}
		{\setlength{\tabcolsep}{0.1em}
		\noindent
		\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}l r r r r@{}}
			\hline
			& \textsc{Surface} &  & \textsc{Roof} & \textsc{Glazing} \\
			\textsc{ID} & \textsc{Area} & \textsc{Height} & \textsc{Area} & \textsc{Area} \\
			\hline
					1 & 784.0 & 3.5 &	220.5 & 0.25	\\
					2 & 710.5 & 3.0 &	210.5 & 0.10	\\
			\hline
		\end{tabular*}
		}
\end{table}

\begin{table}[!hb]
	\caption{The queries for the multivariate logistic regression question}
	\label{tab:logregress}
	\begin{center}
		\begin{tabular}{l r r r}
			\hline
						 & ~ & \textsc{Shop}  & \textsc{Shop}\\
							\textsc{ID} & \textsc{Age} & \textsc{Frequency}  & \textsc{Value}\\
			\hline
			1 & 56 & 1.60 & 109.32 \\
			2 & 21 & 4.92 & 11.28 \\
			3 & 48 & 1.21 & 161.19 \\
			\hline
		\end{tabular}
	\end{center}
\end{table}

\end{document}
