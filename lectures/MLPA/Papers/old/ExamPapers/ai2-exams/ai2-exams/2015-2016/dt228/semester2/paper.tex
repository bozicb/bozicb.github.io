% Filename  : samplepaper.tex
% Purpose   : A sample exam paper to demonstrate how to use the 'ditpaper'
%             TeX class.
% Author    : Emmet Caulfield
% Revision  : $Id: samplepaper.tex 2 2006-02-19 20:34:45Z emmet $
% Repository: $HeadURL: http://svn.netrogen.lan/tex-ditpaper/trunk/samplepaper.tex $
%

% 'nosolution' (default) and 'solution' toggle the inclusion of solutions
% in the output. The tag --SOLUTION-OPTION--, below, is replaced by 'sed' 
% in the Makefile to cause both the paper and the solutions to be produced.
%\documentclass[--SOLUTION-OPTION--]{ditpaper}

\documentclass[--SOLUTION-OPTION--]{ditpaper}
%\documentclass[solution]{ditpaper}
%\documentclass[nosolution]{ditpaper}


%\usepackage{epsf}
\usepackage{fleqn}
%\usepackage{rotating}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{eurosym}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Start newcommand defs taken from aima slides style file %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\def\mysum{\begin{Huge}\mbox{$\Sigma$}\end{Huge}}
\def\myint{\begin{LARGE}\mbox{$\int$}\end{LARGE}}
\def\myprod{\begin{Huge}\mbox{$\Pi$}\end{Huge}}

\newcommand{\smbf}[1]{\mbox{{\smathbold #1}}}
\newcommand{\mbf}[1]{\mbox{{\bf #1}}}

\newcommand{\sr}[1]{\mathrel{\raisebox{-0.6ex}{$\stackrel{#1}{\longrightarrow}$}}}
\newcommand{\srbox}[1]{\sr{\fboxsep=1pt\fbox{$\,{\scriptstyle #1}\,$}}}
\newcommand{\srboxbox}[1]{\sr{\fboxsep=1pt\fbox{\fbox{$\,{\scriptstyle #1}\,$}}}}
\newcommand{\keyword}[1]{{\textbf{#1}}\index{#1}}

\newcommand{\featN}[1]{\textsc{#1}}
\newcommand{\featL}[1]{\textit{`#1'}}

\def\Diff{\mbox{{\it Diff}}}

%%%%%% probability and decision theory
\newcommand{\pv}{\mbf{P}}
\newcommand{\qv}{\mbf{Q}}
\newcommand{\given}{\mid}
\def\transition#1#2{q(#1\rightarrow #2)}
\newcommand{\otherthan}{\overline}
\newcommand{\Parents}{Parents}
\newcommand{\parents}{parents}
\newcommand{\Children}{Children}
\newcommand{\children}{children}
\newcommand{\MarkovBlanket}{MB}
\newcommand{\markovBlanket}{mb}

\def\X{\mbf{X}}
\def\x{\mbf{x}}
\def\sx{\smbf{x}}
\def\Y{\mbf{Y}}
\def\y{\mbf{y}}
\def\sy{\smbf{y}}
\def\E{\mbf{E}}
\def\e{\mbf{e}}
\def\D{\mbf{D}}
\def\d{\mbf{d}}
\def\sbe{\smbf{e}}
\def\sE{\smbf{E}}
\def\T{\mbf{T}}
\def\O{\mbf{O}}
\def\se{\smbf{e}}
\def\Z{\mbf{Z}}
\def\z{\mbf{z}}
\def\sz{\smbf{z}}
\def\F{\mbf{F}}
\def\f{\mbf{f}}
\def\A{\mbf{A}}
\def\B{\mbf{B}}
\def\C{\mbf{C}}
\def\b{\mbf{b}}
\def\m{\mbf{m}}
\def\I{\mbf{I}}
\def\H{\mbf{H}}
\def\zeroes{\mbf{0}}
\def\ones{\mbf{1}}
\def\ev{\mbf{ev}}
\def\fv{\mbf{ev}}
\def\sv{\mbf{sv}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% End newcommand defs taken from aima slides style file %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





% These must be set or bizarre defaults will be used:
\facility{Kevin Street, Dublin 8}
\course{BSc. (Honours)\\Degree in Computer Science}
\examcode{S228/419C \& S211C/419C}
\stage{Year 4}
\session{Summer Examinations 2016}
\title{Artificial Intelligence II [CMPU4011]}
\examiners{Dr. John Kelleher\\
Dr. Deirdre. Lillis\\
Mr. P. Collins (DT228 External)\\
Mr. T. Nolan (DT211 External)}
\examdate{Monday $11^{th}$ May 2016}
\examtime{\centerline{4:00 pm to 6:00 pm}}
\instructions{Question 1 is \textbf{compulsory}\par{} Answer Question 1 (40 marks) \textbf{and}\par{} any 2 Other Questions (30 marks each).}

\begin{document}


%aima chapters 18
% inductive bias, learning theory - supervised/unsupervised, overfitting, lazy/eager learner, classification v regression, false positive v false negatives, linear separability, consistency, evaluation

\question
\begin{enumerate}
	\item What is \textbf{supervised machine learning}?
	\marks{5}
\begin{answer}
Supervised machine learning techniques automatically learn the relationship between a set of \keyword{descriptive features} and a \keyword{target feature} from a set of historical \keyword{instances}. Supervised machine learning is a subfield of machine learning. Machine learning is defined as an automated process that extracts patterns from data. In predictive data analytics applications, we use \keyword{supervised machine learning} to build models that can make predictions based on patterns extracted from historical data.
\end{answer}
	\item Explain what can go wrong when a machine learning classifier uses the wrong \textbf{inductive bias}.
	\marks{5}
\begin{answer}
			\begin{itemize}
				\item If the inductive bias of the learning algorithm constrains the search to only consider simple hypotheses we may have excluded the real function from the hypothesis space. In other words, the true function is \textbf{unrealizable} in the chosen hypothesis space, (i.e., we are \textbf{underfitting}). 
				\item If the inductive bias of the learning algorithm allows the search to consider complex hypotheses, the model may hone in on irrelevant factors in the training set. In other words the model with \textbf{overfit} the training data.
			\end{itemize}
\end{answer}

\item Table \ref{tab:testSetPredictions2}, on the next page, shows the predictions made for a categorical target feature by a model for a test dataset. Based on this test set, calculate the evaluation measures listed below.
\begin{enumerate} 
\item A \textbf{confusion matrix}
\marks{6}
\begin{answer}
The confusion matrix can be written as
\vspace{1em}
\noindent\begin{scriptsize}
\begin{tabular}{c c c  c }
    & &  \multicolumn{2}{c}{Prediction} \\
  & &  \featL{true} &  \featL{false} \\
  \hline
  \multirow{2}{*}{\parbox{1.1cm}{\raggedleft Target}}  & \featL{true} & $8$	&	$1$ \\
  & \featL{false} & $0$	&	$11$ 
\end{tabular}
\end{scriptsize}
\vspace{1em}
\end{answer}
\item The \textbf{misclassification rate}
\marks{4}
\begin{equation*}
misclassification~rate = \frac{\left(FP + FN\right)}{\left(TP + TN + FP + FN\right)} 
\end{equation*}
\begin{answer}
Misclassification rate can be calculated as
\begin{alignat*}{2}
misclassification~rate & = \frac{\left(FP + FN\right)}{\left(TP + TN + FP + FN\right)} \\
 & = \frac{\left(0 + 1\right)}{\left(8 + 11 + 0 + 1\right)} \\ 
 & = 0.05
\end{alignat*}
\end{answer}
\item The \textbf{precision}, \textbf{recall}, and \textbf{F$_1$ measure}
\marks{12}
\begin{alignat*}{2}
precision & = \frac{TP}{\left(TP + FP\right)} \\
recall & = \frac{TP}{\left(TP + FN\right)} \\
F_1~measure & = 2 \times \frac{\left(precision \times recall \right)}{\left(precision + recall\right)} \\
\end{alignat*}
\begin{answer}
We can calculate precision and recall as follows (assuming that the \featL{true} target level is the positive level):
\begin{alignat*}{2}
precision & = \frac{TP}{\left(TP + FP\right)} \\
& = \frac{8}{\left(8 + 0\right)} \\
& = 1.000 \\
recall & = \frac{TP}{\left(TP + FN\right)} \\
& = \frac{8}{\left(8 + 1\right)} \\
& = 0.889
\end{alignat*}
Using these figures, we can calculate the F$_1$ measure as
\begin{alignat*}{2}
F_1~measure & = 2 \times \frac{\left(precision \times recall \right)}{\left(precision + recall\right)} \\
& = 2 \times \frac{\left(1.000 \times 0.889 \right)}{\left(1.000 + 0.889\right)} \\
& = 0.941
\end{alignat*}
\end{answer}
\item The \textbf{average class accuracy (harmonic mean)}. (During this calculation you should round all long floats to 3 places of decimal.)
\marks{8}
\begin{equation*}
average~class~accuracy_{HM} = \displaystyle \frac{1}{\displaystyle \frac{1}{|levels(t)|}\sum_{l \in levels(t)} \frac{1}{recall_l}} \\
\end{equation*}
\begin{answer}
First, we calculate the recall for each target level:
\begin{alignat*}{3}
recall_{\text{\featL{true}}} & = \frac{8}{9} & = 0.889 \\
recall_{\text{\featL{false}}} & = \frac{11}{11} & = 1.000 
\end{alignat*}
Then we can calculate a harmonic mean as
\begin{alignat*}{2}
average~class~accuracy_{HM} & = \displaystyle \frac{1}{\displaystyle \frac{1}{|levels(t)|}\sum_{l \in levels(t)} \frac{1}{recall_l}} \\
& = \displaystyle \frac{1}{\displaystyle \frac{1}{2}\left( \frac{1}{0.889} + \frac{1}{1} \right) } \\
& = 0.941
\end{alignat*}
\end{answer}
\end{enumerate} 
\end{enumerate}
\begin{table}[htb]
	\caption{The predictions made by a model for a categorical target on a test set of 20 instances }
	\label{tab:testSetPredictions2}
\begin{center}
	{\setlength{\tabcolsep}{2em}
	\begin{tabular}{cc}
		\hline
		\begin{minipage}{0.3\textwidth}
			\raggedright
				{\setlength{\tabcolsep}{1em}
			\begin{tabular}{@{\extracolsep{\fill}}c c c@{}}
ID	 & Target & Prediction \\
\hline
1	&	false	&	false \\
2	&	false	&	false \\
3	&	false	&	false \\
4	&	false	&	false \\
5	&	true	&	true \\
6	&	false	&	false \\
7	&	true	&	true \\
8	&	true	&	true \\
9	&	false	&	false \\
10	&	false	&	false \\
\hline
			\end{tabular}
			}
		\end{minipage}
		&
		\begin{minipage}{0.3\textwidth}
			\raggedleft
				{\setlength{\tabcolsep}{1em}
			\begin{tabular}{@{\extracolsep{\fill}}c c c@{}}
ID	 & Target	& Prediction \\
\hline
11	&	false	&	false \\
12	&	true	&	true \\
13	&	false	&	false \\
14	&	true	&	true \\
15	&	false	&	false \\
16	&	false	&	false \\
17	&	true	&	false \\
18	&	true	&	true \\
19	&	true	&	true \\
20	&	true	&	true \\
\hline
			\end{tabular}
			}
		\end{minipage}\\
	\end{tabular}
	}
\end{center}
\end{table}

\clearpage

%Q2
% knn and CBR 
% information theory, entropy, Decision Trees, Inductive logic programming
\newpage

\question 
	\begin{enumerate}
		\item A data analyst building a \emph{k}-nearest neighbour model for a continuous prediction problem is considering appropriate values to use for $k$. 
	\begin{enumerate}
		\item Initially the analyst uses a simple average of the target variables for the $k$ nearest neighbours in order to make a new prediction. After experimenting with values for $k$ in the range $0 - 10$ it occurs to the analyst that they might get very good results if they set $k$ to the total number of instances in the training set. Do you think the analyst is likely to get good results using this value for $k$?
		\marks{5}
\begin{answer}
In answering this question students should realise that if the analyst set $k$ to the number of training examples all predictions would essentially be the average target value across the whole dataset. To score very well students should realise that this is an example of massive underfitting.
\end{answer}
		\item If the analyst was using a distance weighted averaging function rather than a simple average for their predictions would this have made their idea any more useful?
		\marks{5}
\begin{answer}
Students should realise that yes, if distance weighted voting is used (particularly if a $\frac{1}{d^2}$ type distance weight is used) then examples that are far away from the query will have very little impact on the result. Again to score well students should mention that when distance weighted voting is used the value of $k$ in $k$-NN classifiers is much less important.
\end{answer}
	\end{enumerate}
		\item A dataset showing the decisions made by an individual about whether to wait for a table at a restaurant is listed in Table 1 on the next page. (Note that Table \ref{tab:info-eqs}, also on the next page, lists some equations that you may find useful for this question.)		
	\begin{enumerate}
		\item Given that the \textsc{WillWait} column lists the values of the target variable, compute the entropy for this dataset.
			\marks{5}
			\begin{answer}
					There are 6 positive and 6 negative examples in this dataset. This means that the entropy for the dataset is:
					\begin{eqnarray*}
						I(\frac{6}{12}, \frac{6}{12}) &=& -\frac{6}{12}\log_2\frac{6}{12} + -\frac{6}{12}\log_2 \frac{6}{12}\\
									~ &=& (-\frac{1}{2}\log_2\frac{1}{2}) + (-\frac{1}{2}\log_2\frac{1}{2}) \\ 
						~ &=&  -\frac{1}{2}(-1) + -\frac{1}{2}(-1) \\
						 ~ &=&  1bit
					\end{eqnarray*}			
			\end{answer}
		\item What is the information gain for the \textsc{Patrons} feature?
			\marks{5}
			\begin{answer}
							$Gain(Patrons)=1- ( \frac{2}{12}I(0,1) + \frac{4}{12}I(1,0) + \frac{6}{12}I(\frac{2}{6},\frac{4}{6}) ) \approx 0.541$ bits
			\end{answer}
		\item What is the information gain for the \textsc{Type} feature?
			\marks{5}
			\begin{answer}
							$Gain(Type)=1- ( \frac{2}{12}I(\frac{1}{2},\frac{1}{2}) + \frac{1}{2}I(\frac{1}{2},\frac{1}{2}) + \frac{4}{12}I(\frac{2}{4},\frac{2}{4})+ \frac{4}{12}I(\frac{2}{4},\frac{2}{4}) ) = 0$ bits
			\end{answer}
		\item Given a choice between the \textsc{Patrons} and \textsc{Type} feature, which feature would the ID3 algorithm choose as the root node for a decision tree?
			\marks{5}
			\begin{answer}
					The ID3 algorithm would choose the Patrons feature as the root node for the decision tree because it has the higher information gain.
			\end{answer}
	\end{enumerate}
\end{enumerate}

\newpage 

\begin{table}[h]
%\begin{tiny}
\begin{center}
\begin{tabular}{lcccccccc}
\hline
%\textbf{Example} & \multicolumn{5}{c||}{\textbf{Attributes}} & \textbf{Target} \\ 
%\cline{2-6}
\textsc{ID} & \textsc{Bar} & \textsc{Patrons} & \textsc{Price} & \textsc{Rain} & \textsc{Type} & \textsc{WillWait} \\
\hline
$1$  & F & Some & \euro\euro\euro & F & French &  $T$\\
$2$    & F & Full & \euro & F & Thai &  $F$\\
$3$    & T & Some & \euro & F & Burger &  $T$\\
$4$    & F & Full & \euro & F & Thai  &  $T$\\
$5$    & F & Full & \euro\euro\euro & F & French &  $F$\\
$6$    & T & Some & \euro\euro & T & Italian &  $T$\\
$7$    & T & None & \euro & T & Burger &  $F$\\
$8$    & F & Some & \euro\euro & T & Thai &  $T$\\
$9$     & T & Full & \euro & T & Burger &  $F$\\
$10$ & T & Full & \euro\euro\euro & F & Italian &  $F$\\
$11$ & F & None & \euro & F & Thai &  $F$\\
$12$ & T & Full & \euro & F & Burger &  $T$\\
\hline
\end{tabular}
\end{center}
%\end{tiny}
\label{tab:rest}
\caption{A dataset describing the previous decisions made by an individual about whether to wait for a table at a restaurant.}
\end{table}

	\begin{table}[!hb]
			\renewcommand{\arraystretch}{2}
	\begin{center}
	\caption{Equations from information theory.}
	\label{tab:info-eqs}
		\begin{tabular}{ll}
	\hline
	$H(\mathbf{f}, \mathcal{D})$ & $= -\displaystyle\sum_{l \in levels(f)} P(f=l) \times log_2(P(f=l))$\\
	$rem(\mathbf{f}, \mathcal{D})$ & $=\displaystyle\sum_{l \in levels(f)} \frac{|\mathcal{D}_{f=l}|}{|\mathcal{D}|} \times H(t, \mathcal{D})$\\
	$IG(\mathbf{d},\mathcal{D})$ & $=H(\mathbf{t}, \mathcal{D})-rem(\mathbf{d}, \mathcal{D})$\\
	\hline
	\end{tabular}
	\end{center}
	\end{table}		

\newpage

				
\begin{table}[h]
\caption{Query Document}
\centering
\begin{tabular}{l}
\hline
\textit{Machine learning is fun}\\
\hline
\end{tabular}
\label{tab:likedislikequery}
\end{table}

\begin{table}[!htb]
    \caption{Document counts from the corpus for the words in the query.}
    \begin{minipage}{.5\linewidth}
      \centering
Document counts for the \textsc{dislike} data set
\begin{tabular}{|c|c|c|c|}
\hline
fun & is & machine & learning\\
415 & 695 & 35 & 70\\
\hline
\end{tabular}
    \end{minipage}%
    \begin{minipage}{.5\linewidth}
      \centering
Document counts for the \textsc{like} data set
\begin{tabular}{|c|c|c|c|}
\hline
fun & is & machine & learning\\
200 & 295 & 120 & 105\\
\hline
\end{tabular}
    \end{minipage} 
    \label{tab:doccounts}
\end{table}

\question Lets assume we are given a set of \textbf{700} training documents that a friend has classified as \textsc{dislike} and another \textbf{300} documents that they have classified as \textsc{like}. We are now given a new  document and asked to classify it. Table \ref{tab:likedislikequery} lists the content of this query document and Table \ref{tab:doccounts} gives the number of documents from each class (\textsc{dislike} and \textsc{like}) that the words in the query document occurred in. \textbf{What class will a \textbf{Naive Bayes} prediction model label the query document as belonging to?} (You must support your answer by showing the calculations that a Naive Bayes model will make.)
		\marks{30}
		\begin{answer}
			A Naive Bayes model will label the query with the class that has the highest probability under the assumption of conditional independence between the evidence features. So to answer this question we need to calculate the probability of each class given the evidence and assuming conditional independence across the evidence.\\
			To carry out these calculation we need to convert the raw documents counts into conditional probabilities by dividing each count by the total number of documents occuring in class:
\begin{center}
								\textbf{$P(w_k|C=dislike)$}
								\begin{tabular}{|c|c|c|c|}
								\hline
											$w_k$ & Count & $P(w_k | C=dislike)$\\
												\hline
								 fun & 415 & $\frac{415}{700} = .593$\\
								 			is & 695 & $\frac{695}{700} = .99$\\
											learning & 35 & $\frac{35}{700} = .05$\\
											machine & 70 & $\frac{70}{700} = .10$\\
								\hline
								\end{tabular}
\end{center}
\begin{center}
								\textbf{$P(w_k|C=like)$}										
								\begin{tabular}{|c|c|c|c|}
								\hline
								$w_k$ & Count & $P(w_k | C=like)$\\
												\hline
								 fun & 200 & $\frac{200}{300} = .667$\\
								 is & 295 & $\frac{295}{300} = .983$\\
								learning & 120 & $\frac{120}{300} = .40$\\
								machine & 105 & $\frac{105}{300} = .35$\\
								\hline
								\end{tabular}
\end{center}

We can now compute the probabilities of each class:\\

$P(dislike|Query~Document)$
			\begin{eqnarray*}
&=& P(dislike) \times P(Machine|dislike) \times P(learning|dislike) \times P(is|dislike) \times p(fun|dislike).\\
&=& 0.7 \times 0.593 \times 0.99 \times 0.5 \times 0.1\\
&=& 0.00205
			\end{eqnarray*}
$P(like|Query~Document)$
			\begin{eqnarray*}
&=& P(like) \times P(Machine|like) \times P(learning|like) \times P(is|like) \times p(fun|like).\\
&=& 0.3 \times 0.667 \times 0.983 \times 0.4 \times 0.35\\
&=& 0.00275
			\end{eqnarray*}
			
As $P(like|Query~Document) > P(dislike|Query~Document)$ the naive bayes classifier will return a label of \textsc{like}.
		\end{answer}
%
\newpage
\clearpage

%Q4
%Linear Regression Neural Nets, SVMs, Ensemble Learning
%Q4
%Linear Regression Neural Nets, SVMs, Ensemble Learning


\question 
 	\begin{enumerate}	
		\item A multivariate logistic regression model has been built to predict the propensity of shoppers to perform a repeat purchase of a free gift that they are given. The descriptive features used by the model are the age of the customer, the average amount of money the customer spends on each visit to the shop, and the average number of visits the customer makes to the shop per week. This model is being used by the marketing department to determine who should be given the free gift. The trained model is
\begin{alignat*}{2}
\textsc{Repeat~Purchase} = & -3.82398 - 0.02990   \times \textsc{Age} \\ 
& + 0.74572 \times \textsc{Shop~Frequency}\\
& + 0.02999 \times \textsc{Shop~Value}
\end{alignat*}
And, the logistic function is defined as:
\begin{equation*}
	logistic(x)=\frac{1}{1+e^{-x}}
\end{equation*}
Assuming that the \textit{yes} level is the positive level and the classification threshold is $0.5$, use this model to make predictions for each of the query instances shown in Table \ref{tab:logregress}, on the next page.
\marks{12}

		\begin{answer}
		Calculating the predictions made by the model simply involves inserting the descriptive features from each query instance into the prediction model. With this information, the predictions can be made as follows:
				
		\begin{description}
%		\item[1:] $Logistic(-3.82398+-0.0299\times56+0.74572\times1.6+0.02999\times109.32)$ \\
%		$=Logistic(-1.02672) =\frac{1}{1-e^{1.02672}}$\\
%		$=0.26372 \Rightarrow \textit{no}$
%		\item[2:] $Logistic(-3.82398+-0.0299\times21+0.74572\times4.92+0.02999\times11.28)$ \\
%		$=Logistic(-0.44465)  =\frac{1}{1-e^{0.44465}}$ \\
%		$=0.390633 \Rightarrow \textit{no}$
%		\item[3:] $Logistic(-3.82398+-0.0299\times48+0.74572\times1.21+0.02999\times161.19)$ \\
%		$=Logistic(0.477229)   =\frac{1}{1-e^{-0.477229}}$ \\
%		$=0.6205 \Rightarrow \textit{yes}$
		\item[A:] $Logistic(-3.82398+-0.0299\times37+0.74572\times0.72+0.02999\times170.65)$ \\
		$=Logistic(0.724432)   =\frac{1}{1-e^{0.-724432}}$ \\
		$=0.673582 \Rightarrow \textit{yes}$
		\item[B:] $Logistic(-3.82398+-0.0299\times32+0.74572\times1.08+0.02999\times165.39)$ \\
		$=Logistic(0.984644)   =\frac{1}{1-e^{-0.98464}}$ \\
		$=0.728029 \Rightarrow \textit{yes}$
		\end{description}
		\end{answer}

		\item The effects that can occur when different drugs are taken together can be difficult for doctors to predict. A machine learning has been trained to distinguish between dosages of two drugs that cause a dangerous interaction and those that cause a safe interaction. There are just two continuous features in this dataset, \textsc{Dose1} and \textsc{Dose2}, and two target levels, \textit{dangerous} and \textit{safe}. There is a non-linear decision boundary between dangerous and safe interactions and, consequently, the following set of basis functions were defined:
	\begin{center}
%\begin{adjustwidth}{-2.5pc}{0pc}
\begin{tabular}[ht]{ l l }
$\phi_0(\mathbf{\left<\textsc{Dose1}, \textsc{Dose2}\right>}) =  1$ &
$\phi_1(\left<\textsc{Dose1}, \textsc{Dose2}\right>) = \textsc{Dose1}$  \\
$\phi_2(\left<\textsc{Dose1}, \textsc{Dose2}\right>) = \textsc{Dose2}$  &
$\phi_3(\left<\textsc{Dose1}, \textsc{Dose2}\right>) = \textsc{Dose1}^2$ \\
$\phi_4(\left<\textsc{Dose1}, \textsc{Dose2}\right>) = \textsc{Dose2}^2$ &
$\phi_5(\left<\textsc{Dose1}, \textsc{Dose2}\right>) = \textsc{Dose1}^3$ \\
$\phi_6(\left<\textsc{Dose1}, \textsc{Dose2}\right>) = \textsc{Dose2}^3$ &
 $\phi_7(\left<\textsc{Dose1}, \textsc{Dose2}\right>) = \textsc{Dose1} \times \textsc{Dose2}$ \\
\end{tabular}
%\end{adjustwidth}
\end{center}
Training a logistic regression model using this set of basis functions leads to the following model:
%\begin{adjustwidth}{-2.5pc}{0pc}
			\begin{alignat*}{2}				
P(\textsc{Type}&=\textit{dangerous}) = \\
 Logistic\big(&-0.848\times \phi_0(\mathbf{\left<\textsc{Dose1}, \textsc{Dose2}\right>}) 
  + 1.545 \times\phi_1(\mathbf{\left<\textsc{Dose1}, \textsc{Dose2}\right>})   \\
 & -1.942 \times\phi_2(\mathbf{\left<\textsc{Dose1}, \textsc{Dose2}\right>})
  + 1.973 \times\phi_3(\mathbf{\left<\textsc{Dose1}, \textsc{Dose2}\right>}) \\
 & +  2.495   \times  \phi_4(\mathbf{\left<\textsc{Dose1}, \textsc{Dose2}\right>})
   + 0.104 \times\phi_5(\mathbf{\left<\textsc{Dose1}, \textsc{Dose2}\right>})   \\
  & + 0.095 \times\phi_6(\mathbf{\left<\textsc{Dose1}, \textsc{Dose2}\right>})
  + 3.009\times\phi_7(\mathbf{\left<\textsc{Dose1}, \textsc{Dose2}\right>})\big) 
\end{alignat*}
%\end{adjustwidth}
Use this model to make predictions for the query instances in Table \ref{tab:dosagePrd} and using these prediction explain whether or not the dosage combinations are likely to lead to a dangerous of safe interaction.
\marks{18}
		\begin{answer}
	The first step in making a prediction is to generate the outputs of the basis functions. This is done for the first query as follows:
			\begin{center}
\begin{tabular}[ht]{ l  l  }
$\phi_0(\left<0.50, 0.75\right>) =  1$ & $\phi_4(\left<0.50, 0.75\right>) = 0.5625$ \\
$\phi_1(\left<0.50, 0.75\right>) = 0.50$ & $\phi_5(\left<0.50, 0.75\right>) = 0.1250$ \\
$\phi_2(\left<0.50, 0.75\right>) = 0.75$ &$\phi_6(\left<0.50, 0.75\right>) = 0.4219$  \\
$\phi_3(\left<0.50, 0.75\right>) = 0.25$ & $\phi_7(\left<0.50, 0.75\right>) = 0.3750$ \\
\end{tabular}
\end{center}
	We can now use the regression model to make a prediction:
		\begin{alignat*}{2}				
P(&\textsc{Type}=\textit{dangerous}) \\
& = Logistic(-0.848\times 1 + 1.545 \times0.50 -1.942 \times0.75 + 1.973 \times0.25 \\
& ~~~ +  2.495   \times 0.5625 + 0.104 \times 0.1250 + 0.095 \times 0.4219 + 3.009\times0.3750) \\
 & = Logistic(1.5457)  \\
 & = 0.8243 
\end{alignat*}

\noindent This means that the probability of the query dosages causing a \textit{dangerous} interaction is $0.8243$, so we would say that the result for this query is  \textit{dangerous} . 

For the next query $\left<0.10	,0.75\right>$:
			\begin{center}
\begin{tabular}[ht]{ l  l  }
$\phi_0(\left<0.10, 0.75\right>) =  1$ & $\phi_4(\left<0.10, 0.75\right>) = 0.5625$ \\
$\phi_1(\left<0.10, 0.75\right>) = 0.10$ & $\phi_5(\left<0.10, 0.75\right>) = 0.0010$ \\
$\phi_2(\left<0.10, 0.75\right>) = 0.75$ &$\phi_6(\left<0.10, 0.75\right>) = 0.4219$  \\
$\phi_3(\left<0.10, 0.75\right>) = 0.01$ & $\phi_7(\left<0.10, 0.75\right>) = 0.0750$ \\
\end{tabular}
\end{center}
	We can now use the regression model to make a prediction:
		\begin{alignat*}{2}				
P(&\textsc{Type}=\textit{dangerous}) \\
&= Logistic(-0.848\times 1 + 1.545 \times 0.10  -1.942 \times0.75 + 1.973 \times 0.01 \\
& ~~~   +  2.495   \times 0.5625 + 0.104 \times 0.0010 + 0.095 \times 0.4219 + 3.009\times 0.0750) \\
& =  Logistic(-0.4613)  \\
& = 0.3867 
\end{alignat*}
 
\noindent This means that the probability of the query dosages causing \textit{dangerous} interaction is $0.3867$, so we would say that these dosages are \textit{safe} together. 

%And for the next query $\left<-0.47, -0.5\right>$:
%			\begin{center}
%\begin{tabular}[ht]{ l  l  }
%$\phi_0(\left<-0.47, -0.50\right>) =  1$ & $\phi_4(\left<-0.47, -0.50\right>) = 0.2500$ \\
%$\phi_1(\left<-0.47, -0.50\right>) = -0.47$ & $\phi_5(\left<-0.47, -0.50\right>) = -0.1038$ \\
%$\phi_2(\left<-0.47, -0.50\right>) = -0.50$ &$\phi_6(\left<-0.47, -0.50\right>) = -0.1250$  \\
%$\phi_3(\left<-0.47, -0.50\right>) = 0.2209$ & $\phi_7(\left<-0.47, -0.50\right>) = 0.2350$ \\
%\end{tabular}
%\end{center}
%	We can now use the regression model to make a prediction:
%		\begin{alignat*}{2}				
%P(&\textsc{Type}=\textit{dangerous}) \\
%& = Logistic(-0.848\times 1 + 1.545 \times -0.47 -1.942 \times  -0.50 + 1.973 \times 0.2209 \\
%&  ~~~  +  2.495   \times 0.25 + 0.104 \times -0.1038 + 0.095 \times -0.1250 + 3.009\times 0.2350) \\
%& =  Logistic(1.1404)  \\
%& = 0.7577 
%\end{alignat*}
% 
%\noindent This means that the probability of the query document causing a \textit{dangerous} interaction is $0.7577$, so we would return a \textit{dangerous} prediction.

And for the last query $\left<-0.47, 0.18\right>$:
			\begin{center}
\begin{tabular}[ht]{ l  l  }
$\phi_0(\left<-0.47, 0.18\right>) =  1$ & $\phi_4(\left<-0.47, 0.18\right>) = 0.0324$ \\
$\phi_1(\left<-0.47, 0.18\right>) = -0.47$ & $\phi_5(\left<-0.47, 0.18\right>) = -0.1038$ \\
$\phi_2(\left<-0.47, 0.18\right>) = 0.18$ &$\phi_6(\left<-0.47, 0.18\right>) = 0.0058$  \\
$\phi_3(\left<-0.47, 0.18\right>) = 0.2209$ & $\phi_7(\left<-0.47, 0.18\right>) = -0.0846$ \\
\end{tabular}
\end{center}
	We can now use the regression model to make a prediction:
		\begin{alignat*}{2}				
P(&\textsc{Type}=\textit{dangerous}) \\
& = Logistic(-0.848\times 1 + 1.545 \times -0.47 -1.942 \times  0.18 + 1.973 \times 0.2209 \\
&  ~~~  +  2.495   \times 0.0324 + 0.104 \times -0.1038 + 0.095 \times 0.0058 + 3.009\times -0.0846) \\
& = Logistic(-1.672106798)  \\
& = 0.1581 
\end{alignat*}
 
\noindent This means that the probability of the query dosages causing a \textit{dangerous} interaction is $0.1581$, so we would say that, instead, this is a \textit{safe} dosage pair. 
		\end{answer}
		\end{enumerate}

\clearpage

%\begin{footnotesize}
%{\setlength{\tabcolsep}{0.1em}
%\noindent\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}} l r r r@{}}
\begin{table}[htb]
\caption{The queries for the multivariate logistic regression question}
\label{tab:logregress}
\begin{center}
\begin{tabular}{l r r r}
\hline
			 & ~ & \textsc{Shop}  & \textsc{Shop}\\
				\textsc{ID} & \textsc{Age} & \textsc{Frequency}  & \textsc{Value}\\
\hline
%1 & 56 & 1.60 & 109.32 \\ 
%2 & 21 & 4.92 & 11.28 \\  
%3 & 48 & 1.21 & 161.19 \\ 
A & 37 & 0.72 & 170.65 \\ 
B & 32 & 1.08 & 165.39 \\ 
\hline
\end{tabular}
\end{center}
\end{table}
%\end{tabular*}{}
%}
%\end{footnotesize}
%\end{adjustwidth}

\begin{table}[h]
\begin{center}
\caption{The query instances for the dosage prediction problem}
\label{tab:dosagePrd}
\noindent\begin{tabular}{@{} l r r@{}}
\hline
				\textsc{ID} & \textsc{Dose1} & \textsc{Dose2} \\
\hline
		1 & 0.50 & 0.75 \\
		2 & 0.10 & 0.75 \\
%		3 & -0.47 & -0.39 \\
		3 & -0.47 & 0.18 \\
\hline
\end{tabular}
\end{center}
\end{table}


\end{document}
