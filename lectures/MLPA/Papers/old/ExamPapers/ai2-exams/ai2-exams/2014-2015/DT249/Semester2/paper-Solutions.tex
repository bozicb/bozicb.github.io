% Filename  : samplepaper.tex
% Purpose   : A sample exam paper to demonstrate how to use the 'ditpaper'
%             TeX class.
% Author    : Emmet Caulfield
% Revision  : $Id: samplepaper.tex 2 2006-02-19 20:34:45Z emmet $
% Repository: $HeadURL: http://svn.netrogen.lan/tex-ditpaper/trunk/samplepaper.tex $
%

% 'nosolution' (default) and 'solution' toggle the inclusion of solutions
% in the output. The tag solution, below, is replaced by 'sed' 
% in the Makefile to cause both the paper and the solutions to be produced.
%\documentclass[solution]{ditpaper}

\documentclass[solution]{ditpaper}
%\documentclass[solution]{ditpaper}

%\usepackage{epsf}
\usepackage{fleqn}
%\usepackage{rotating}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsmath}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Start newcommand defs taken from aima slides style file %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\def\mysum{\begin{Huge}\mbox{$\Sigma$}\end{Huge}}
\def\myint{\begin{LARGE}\mbox{$\int$}\end{LARGE}}
\def\myprod{\begin{Huge}\mbox{$\Pi$}\end{Huge}}

\newcommand{\smbf}[1]{\mbox{{\smathbold #1}}}
\newcommand{\mbf}[1]{\mbox{{\bf #1}}}

\newcommand{\sr}[1]{\mathrel{\raisebox{-0.6ex}{$\stackrel{#1}{\longrightarrow}$}}}
\newcommand{\srbox}[1]{\sr{\fboxsep=1pt\fbox{$\,{\scriptstyle #1}\,$}}}
\newcommand{\srboxbox}[1]{\sr{\fboxsep=1pt\fbox{\fbox{$\,{\scriptstyle #1}\,$}}}}

\def\Diff{\mbox{{\it Diff}}}

%%%%%% probability and decision theory
\newcommand{\pv}{\mbf{P}}
\newcommand{\qv}{\mbf{Q}}
\newcommand{\given}{\mid}
\def\transition#1#2{q(#1\rightarrow #2)}
\newcommand{\otherthan}{\overline}
\newcommand{\Parents}{Parents}
\newcommand{\parents}{parents}
\newcommand{\Children}{Children}
\newcommand{\children}{children}
\newcommand{\MarkovBlanket}{MB}
\newcommand{\markovBlanket}{mb}

\def\X{\mbf{X}}
\def\x{\mbf{x}}
\def\sx{\smbf{x}}
\def\Y{\mbf{Y}}
\def\y{\mbf{y}}
\def\sy{\smbf{y}}
\def\E{\mbf{E}}
\def\e{\mbf{e}}
\def\D{\mbf{D}}
\def\d{\mbf{d}}
\def\sbe{\smbf{e}}
\def\sE{\smbf{E}}
\def\T{\mbf{T}}
\def\O{\mbf{O}}
\def\se{\smbf{e}}
\def\Z{\mbf{Z}}
\def\z{\mbf{z}}
\def\sz{\smbf{z}}
\def\F{\mbf{F}}
\def\f{\mbf{f}}
\def\A{\mbf{A}}
\def\B{\mbf{B}}
\def\C{\mbf{C}}
\def\b{\mbf{b}}
\def\m{\mbf{m}}
\def\I{\mbf{I}}
\def\H{\mbf{H}}
\def\zeroes{\mbf{0}}
\def\ones{\mbf{1}}
\def\ev{\mbf{ev}}
\def\fv{\mbf{ev}}
\def\sv{\mbf{sv}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% End newcommand defs taken from aima slides style file %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





% These must be set or bizarre defaults will be used:
\facility{Kevin Street, Dublin 8}
\course{BSc. (Honours)\\Degree in Information Systems /\\ Information Technology}
\examcode{S249/419C}
\stage{Stage 4}
\session{Summer Examinations 2015}
\title{Artificial Intelligence II [CMPU4011]}
\examiners{Dr. John Kelleher\\
Dr. Deirdre. Lillis\\
Dr. Rem Collier}
\examdate{Monday $11^{th}$ May 2015}
\examtime{\centerline{4:00 p.m to 6:00 p.m}}
\instructions{Question 1 is \textbf{compulsory}\par{} Answer Question 1 (40 marks) \textbf{and}\par{} any 2 Other Questions (30 marks each).}

\begin{document}


%aima chapters 18
% inductive bias, learning theory - supervised/unsupervised, overfitting, lazy/eager learner, classification v regression, false posi	tive v false negatives, linear separability, consistency, evaluation

\question
\begin{enumerate}
	\item Explain what is meant by \textbf{inductive learning}.
	\marks{5}
	\begin{answer}
		Inductive Learning involves the process of learning by example where a system tries to induce a general rule from a set of observed instances.
	\end{answer}
	\item Explain what can go wrong when a machine learning classifier uses the wrong inductive bias.
	\marks{5}
\begin{answer}
			\begin{itemize}
				\item If the inductive bias of the learning algorithm constrains the search to only consider simple hypotheses we may have excluded the real function from the hypothesis space. In other words, the true function is \textbf{unrealizable} in the chosen hypothesis space, (i.e., we are \textbf{underfitting}). 
				\item If the inductive bias of the learning algorithm allows the search to consider complex hypotheses, the model may hone in on irrelevant factors in the training set. In other words the model with \textbf{overfit} the training data.
			\end{itemize}
\end{answer}

	\item Table \ref{tab:testSetPredictions} shows the predictions made for a categorical target feature by a model for a test dataset. 
\begin{enumerate} 
\item Create the \textbf{confusion matrix} for the results listed in Table \ref{tab:testSetPredictions}.
\marks{5}
\begin{answer}
%\begin{tabular}{c >{}r @{\hspace{0.7em}} !{\color{black}\vrule} c @{\hspace{0.4em}} c @{\hspace{0.7em}}}
\begin{tabular}{c c c  c }
   ~ & ~ &  \multicolumn{2}{c}{Prediction} \\
  ~ & ~ &  \textit{true} &  \textit{false} \\
  \hline
  \multirow{2}{*}{\parbox{1.1cm}{\raggedleft Target}}  & \textit{true} & $1$	&	$3$ \\
  & \textit{false} & $2$	&	$14$ 
\end{tabular}
\end{answer}

\item Calculate the \textbf{classification accuracy} for the results listed in Table \ref{tab:testSetPredictions}.
\begin{equation*}
classification~accuracy = \frac{\left(TP + TN\right)}{\left(TP + TN + FP + FN\right)}
\end{equation*}
\marks{5}
\begin{answer}
Classification accuracy can be calculated as
\begin{alignat*}{2}
classification~rate & = \frac{\left(TP + TN\right)}{\left(TP + TN + FP + FN\right)} \\
 & = \frac{\left(1 + 14\right)}{\left(1 + 14 + 3 + 2\right)} \\ 
 & = 0.75
\end{alignat*}
\end{answer}

\item Calculate the \textbf{average class accuracy (harmonic mean)} for the results listed in Table \ref{tab:testSetPredictions}. (During this calculation you should round all long floats to 4 places of decimal.)
\begin{equation*}
average~class~accuracy_{HM} = \displaystyle \frac{1}{\displaystyle \frac{1}{|levels(t)|}\sum_{l \in levels(t)} \frac{1}{recall_l}} \\
\end{equation*}
\marks{8}
\begin{answer}
Note, in this solution we round all figures to four places of decimal.
First, we calculate the recall for each target level:
\begin{alignat*}{3}
recall_{\textit{true}} & = \frac{1}{4} & = 0.25 \\
recall_{\textit{false}} & = \frac{14}{16} & = 0.875
\end{alignat*}
Then we can calculate a harmonic mean as
\begin{alignat*}{2}
average~class~accuracy_{HM} & = \displaystyle \frac{1}{\displaystyle \frac{1}{|levels(t)|}\sum_{l \in levels(t)} \frac{1}{recall_l}} \\
& = \displaystyle \frac{1}{\displaystyle \frac{1}{2}\left( \frac{1}{0.25} + \frac{1}{0.875} \right) } \\
& = \displaystyle \frac{1}{\displaystyle \frac{1}{2}\left( 4 + 1.1429 \right) } \\
& = 0.38889
\end{alignat*}
\end{answer}

\item Which of these performance metrics (\textbf{misclassification rate} or \textbf{average class accuracy (harmonic mean)} is the most appropriate metric to use in this scenario? Provide an explanation for your answer.
\marks{12}
	\begin{answer}
This is a discursive question so providing a precise answer is not appropriate, but in general the students should:
	\begin{itemize}
		\item Note that the test dataset is \textbf{imbalanced}: there are only 4 instances of the \textit{true} class in the test dataset.
		\item Point out that classification accuracy does not take into account the distribution of classes in the test set, and as a result it can hide poor performance of a model on one of the class. In this particular instance, the classification accuracy is dominated by the performance of the model on the false class and as a result hides the fact that the model only gets 1 out of the four true instances correct.
		\item Average class accuracy (harmonic mean) does take the distribution of class into account and furthermore by using the harmonic mean also pays more attention to the classes that the model has low recall on. As a result average class accuracy is the more appropriate performance metric to use in this instance. 
	\end{itemize}
	\end{answer}
	\end{enumerate}
\end{enumerate} 
\begin{table}[htb]
	\caption{The predictions made by a model for a categorical target on a test set of 20 instances  }
	\label{tab:testSetPredictions}
\begin{center}
	{\setlength{\tabcolsep}{2em}
	\begin{tabular}{cc}
		\hline
		\begin{minipage}{0.3\textwidth}
			\raggedright
				{\setlength{\tabcolsep}{1em}
			\begin{tabular}{@{\extracolsep{\fill}}c c c@{}}
ID	 & Target & Prediction \\
\hline
1	&	false	&	false \\
2	&	false	&	false \\
3	&	false	&	false \\
4	&	false	&	false \\
5	&	false	&	true \\
6	&	false	&	false \\
7	&	false	&	false \\
8	&	false	&	false \\
9	&	false	&	false \\
10	&	false	&	false \\
\hline
			\end{tabular}
			}
		\end{minipage}
		&
		\begin{minipage}{0.3\textwidth}
			\raggedleft
				{\setlength{\tabcolsep}{1em}
			\begin{tabular}{@{\extracolsep{\fill}}c c c@{}}
ID	 & Target	& Prediction \\
\hline
11	&	false	&	false \\
12	&	false	&	true \\
13	&	false	&	false \\
14	&	false	&	false \\
15	&	false	&	false \\
16	&	false	&	false \\
17	&	true	&	false \\
18	&	true	&	false \\
19	&	true	&	false \\
20	&	true	&	true \\
\hline
			\end{tabular}
			}
		\end{minipage}\\
	\end{tabular}
	}
\end{center}
\end{table}

\clearpage

%Q2
% knn and CBR 
% information theory, entropy, Decision Trees, Inductive logic programming
\question 
	\begin{enumerate}	
		\item Table \ref{tab:3nn-data}, on the next page, lists a dataset containing examples described by two descriptive features, \textbf{Feature 1} and \textbf{Feature 2}, and labelled with a target class \textbf{Target}. Table \ref{tab:3nn-query}, also on the next page, lists the details of a query for which we want to predict the target label. We have decided to use a \textbf{3-Nearest Neighbor} model for this prediction and we will use Euclidean distance as our distance metric: 
								\begin{center}
								$d(x_1,x_2)=\sqrt{\sum_{i=1}^{n} \left(\left(x_1.f_i - x_2.f_i \right)^2 \right)}$
								\end{center}					
		\begin{enumerate}
				\item With which target class (\textbf{C1}  or \textbf{C2}) will our \textbf{3-Nearest Neighbor} model label the query? Provide an explanation for your answer.				
			  \marks{8}
				\begin{answer}
					The first stage is to calculate the Euclidean distance between each of the examples and the query:
					\begin{center}
						\begin{tabular}{|c|c|}
						ID & Euclidean Distance \\
						\hline
						101 & 60000\\
						102 & 120000\\
						103 & 120000\\
						104 & 180000\\
						105 & 240000\\
						\hline
						\end{tabular}
					\end{center}
				 From this table we can see that the three closest examples to the query are examples 101, 102, and 103. Example 101 has a target label of C1 and both 102 and 103 have target labels C2. Consequently C2 is the majority label in local model constructed by the 3-Nearest Neighbor classifier for this query instance and the query will be labelled with class C2.
				\end{answer}
			\item There is a large variation in range between \textbf{Feature 1} and \textbf{Feature 2}. To account for this we decide to normalize the data. Compute the normalized versions of Feature 1 and Feature 2  to four decimal places of precision using range normalization 
								\begin{center}
								$x_i.f^\prime=\frac{x_i.f - min(f)}{max(f)-min(f)}$
								\end{center}		
				\marks{4}
				\begin{answer}
					\begin{center}
\begin{tabular}{cccc}
\hline
ID & Feature 1 & Feature 2  & Target \\
\hline
101 &	0.2 &	 0.1667 &	C1\\
102 &	0    & 	0.0000 &	C2\\
103 &	0.8 & 	0.6667 &	C2\\
104 &	0.4 &	   0.8333 &	C1\\
105 &	1    & 	1.0000 &	C2\\
\hline
\end{tabular}
					\end{center}
				\end{answer}
			\item Assuming we use the normalized dataset as input, with which target class (\textbf{C1}  or \textbf{C2}) will our \textbf{3-Nearest Neighbor} model label the query? Provide an explanation for your answer.				
			  \marks{8}
				\begin{answer}
					The normalize query instance is:
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
ID & Feature 1 & Feature 
2  & Target \\
\hline
250 & 0.2 & 0.3333 & ?\\
\hline
\end{tabular}
\end{center}
				The Euclidean distances between the normalized data and normalized query are: 
					\begin{center}
						\begin{tabular}{|c|c|}
						ID & Euclidean Distance \\
						\hline
101	& 0.1667\\
102	& 0.3887\\
103	& 0.6864\\
104	& 0.5385\\
105	&1.0414\\						
\hline
						\end{tabular}
					\end{center}
From this table we can see that the 3 closest neighbors are: 101, 103 and 104. 101 and 104 are both labelled as class \textbf{C1}. So \textbf{C1} is the majority class in the neighborhood and the query will be labelled as belonging to it.					
				\end{answer}	
		\end{enumerate}
		% Information theory and Decision Trees
		
		\item Table \ref{tab:classification-data}, on the next page, lists a classification dataset. Each instance in the dataset has two descriptive features (Feature A and Feature B) and is classified as either a positive (+) or a negative(-) example. Note that Table \ref{tab:info-eqs}, below, lists some equations that you may find useful for this question.		
			\begin{table}[htb]
			\renewcommand{\arraystretch}{2}
	\begin{center}
	\caption{Equations from information theory.}
	\label{tab:info-eqs}
		\begin{tabular}{ll}
	\hline
	$H(\mathbf{f}, \mathcal{D})$ & $= -\displaystyle\sum_{l \in levels(f)} P(f=l) \times log_2(P(f=l))$\\
	$rem(\mathbf{f}, \mathcal{D})$ & $=\displaystyle\sum_{l \in levels(f)} \frac{|\mathcal{D}_{f=l}|}{|\mathcal{D}|} \times H(t, \mathcal{D})$\\
	$IG(\mathbf{d},\mathcal{D})$ & $=H(\mathbf{t}, \mathcal{D})-rem(\mathbf{d}, \mathcal{D})$\\
	\hline
	\end{tabular}
	\end{center}
	\end{table}
				\begin{enumerate}
			\item Calculate the classification \textbf{entropy} for this dataset.
			\marks{5}
			\begin{answer}
				Entropy is $-\frac{3}{5}log_2\frac{3}{5}-\frac{2}{5}log_2\frac{2}{5}=0.971$
			\end{answer}
			\item Calculate the \textbf{information gain} for Feature A and Feature B.
			\marks{5}
			\begin{answer}
				Entropy for feature A = T $-\frac{3}{4}log_2\frac{3}{4}-\frac{1}{4}log_2\frac{1}{4}=0.811$\\
				Entropy for feature A = F $0-\frac{1}{1}log_2\frac{1}{1}=0$\\
				Gain for feature A $0.971-(\frac{4}{5}\times0.811+\frac{1}{5}\times0)=0.322$\\
				Entropy for feature B = T $-\frac{2}{3}log_2\frac{2}{3}-\frac{1}{3}log_2\frac{1}{3}=0.918$\\
				Entropy for feature B = F $-\frac{1}{2}log_2\frac{1}{2}-\frac{1}{2}log_2\frac{1}{2}=1.0$\\
				Gain for feature B $0.971-(\frac{3}{5}\times0.918+\frac{2}{5}\times1)=0.02$\\
			\end{answer}
		\end{enumerate}
	\end{enumerate}

\newpage

\begin{table}[htdp]
\caption{Dataset for the 3-Nearest Neighbor question}
\begin{center}
\begin{tabular}{cccc}
\hline
ID & Feature 1 & Feature 2  & Target \\
\hline
101 & 4 &	180000 & C1\\
102 & 3 &	120000 & C2\\
103 & 7 &	360000 & C2\\
104 & 5 &	420000 &	C1\\
105 & 8 &	480000 &	C2\\
\hline
\end{tabular}
\end{center}
\label{tab:3nn-data}
\end{table}%

\begin{table}[htdp]
\caption{Query instance for the 3-Nearest Neighbor question.}
\begin{center}
\begin{tabular}{cccc}
\hline
ID & Feature 1 & Feature 2  & Target \\
\hline
250 & 4 &	240000 & ?\\
\hline
\end{tabular}
\end{center}
\label{tab:3nn-query}
\end{table}%


			\begin{table}[htb]
			\caption{Classification dataset for information question.}
			\label{tab:classification-data}
			\begin{center}
			\begin{tabular}{ccc}
				\hline
				Feature A & Feature B & Classification \\
				\hline
				True & True & + \\
				True & False & - \\
				True & False & + \\
				True & True & + \\
				False & True & - \\
				\hline
			\end{tabular}
			\end{center}
		\end{table}

\clearpage



%	\begin{table}[htb]
%	\begin{center}
%	\begin{tabular}{rl}
%	Entropy(DS) & $\displaystyle = -\sum_{i=1}^k p_i \times log_2(p_i)$\\
%	Remainder(F) & $\displaystyle =\sum_{v \in Domain(F)} \frac{|DS_v|}{|DS|} Entropy(DS_v)$\\
%	InformationGain(F,DS) & $\displaystyle =Entropy(DS)-Remainder(F)$\\
%	\end{tabular}
%	\end{center}
%	\caption{Equations from information theory.}
%	\label{tab:info-eqs}
%	\end{table}

%\newpage

				
%Q3 30 marks
% basic probability 5
% bayesian networks 10
% bayesian learning  15

\question Table \ref{tab:rest} lists a dataset of the previous decision made by a couple regarding whether or not they would wait for a table at a restaurant (i.e., the feature \textsc{Waited} is the target feature in this domain). 
	\begin{enumerate}
		\item Calculate the probabilities (to four places of decimal) that a \textbf{naive Bayes} classifier would use to represent this domain.
		\marks{18}
		\begin{answer}
		A naive Bayes classifier would require the prior probability for each level of the target feature and the conditional probability for each level of each descriptive feature given each level of the target feature:
		\begin{footnotesize}
		\begin{tabular}{ll}
		$P(Waited=Yes)=0.4$ & $P(Waited=No)=0.6$\\
		$P(Bar=True|Waited=Yes)=0.5 $& $P(Bar=True|Waited=No)=0.5$\\
		$P(Bar=False|Waited=Yes)=0.5 $& $P(Bar=False|Waited=No)=0.5$\\
		$P(Patrons=None\mid Waited=Yes)= 0.25$ & $P(Patrons=None\mid Waited=No)=0.1667$\\
		$P(Patrons=Some\mid Waited=Yes)= 0.5$ & $P(Patrons=Some\mid Waited=No)=0.3333$\\
		$P(Patrons=Full\mid Waited=Yes)= 0.25$ & $P(Patrons=Full\mid Waited=No)=0.5$\\
		$P(Price=Cheap\mid Waited=Yes)= 0.5$ & $P(Price=Cheap\mid Waited=No)=0.5$\\
		$P(Price=Reasonable\mid Waited=Yes)= 0.25$ & $P(Price=Reasonable\mid Waited=No)=0.3333$\\
		$P(Price=Expensive\mid Waited=Yes)= 0.25$ & $P(Price=Expensive\mid Waited=No)=0.1667$\\
		\end{tabular}
		\end{footnotesize}
		\end{answer}
		\item Assuming conditional independence between features given the target feature value, calculate the \textbf{probability} of each outcome (\textsc{Waited}=Yes, and \textsc{Waited}=No) for the following restaurant for this couple (marks will be deducted if workings are not shown, round your results to four places of decimal)\\
		\begin{center}
		\textsc{Bar}=False, \textsc{Patrons}=None, \textsc{Price}=Expensive
		\end{center}
		\marks{10}
		\begin{answer}
		The initial score for each outcome is calculated as follows:\\
		$(Waited=Yes) =  0.5 \times 0.25 \times 0.25 \times 0.4 = 0.0125$\\
		$(Waited=No) =  0.5 \times 0.1667 \times 0.1667 \times 0.6 = 0.0083$\\
		However, these scores are not probabilities. To get real probabilities we must normalise these scores. The normalisation constant is calculated as follows:\\
		$\alpha=0.0125+0.0083=0.0208$\\
		The actual probabilities of each outcome is then calculated as:
		$P(Waited=Yes) =  \frac{0.0125}{0.0208}=(0.600961...)=0.6010$ \\
		$P(Waited=No) =  \frac{0.0083}{0.0208}=(0.399038...)=0.3990$\\
		\end{answer}
		\item What prediction would a \textbf{naive Bayes} classifier return for the 	above restaurant?			\marks{2}
		\begin{answer}
		A naive Bayes classifier returns outcome with the maximum a posteriori probability as its prediction. In this instance the outcome \textsc{Waited}=Yes is the MAP prediction and will be the outcome returned by a naive Bayes model.
		\end{answer} 
\end{enumerate}
\begin{table}[h]
%\begin{tiny}
\begin{center}
\caption{A dataset describing the previous decisions made by an individual about whether to wait for a table at a restaurant.}
\label{tab:rest}
\vspace{0.5em}
\begin{tabular}{lcccc}
\hline
 ID & \textsc{Bar} & \textsc{Patrons} & \textsc{Price} & \textsc{Waited} \\
\hline
$1$  & False & Some & Expensive &  Yes\\
$2$    & False & Full & Cheap &   No\\
$3$    & True & Some & Cheap &   Yes\\
$4$    & False & Full & Cheap &   Yes\\
$5$    & False & Full & Expensive &   No\\
$6$    & True & Some & Reasonable &   No\\
$7$    & True & None & Cheap &   No\\
$8$    & False & Some & Reasonable &   No\\
$9$     & True & Full & Cheap &   No\\
$10$ & True & None & Reasonable &   Yes\\
\hline
\end{tabular}
\end{center}
%\end{tiny}
\end{table}

\clearpage

%Q4
%Linear Regression Neural Nets, SVMs, Ensemble Learning

\question 
	\begin{enumerate}
\question The following model is commonly used for continuous prediction tasks:

\begin{center}
$y(x)=w_0 + w_1x_1 + \dots + w_Dx_D$
\end{center}

\begin{enumerate}
\item Provide the name for this model and explain all of the terms that it contains.
\marks{4}

		\begin{answer}
		Students should explain that this is a simple linear regression model which can be effectively used to make predictions. $x$ is a vector of feature values for a query instance and $w$ is a vector of feature weights. An diagram of a simple one dimensional linear function would help.
		\end{answer}
				
\item Explain how the following model can overcome some of the limitations of the model given above. 
	\marks{8}

\begin{center}
$\displaystyle y(x)=\sum_{j=0}^{M - 1}w_j{\phi}_j(x)$
\end{center}
\end{enumerate} 

		\begin{answer}
		Students should explain that the simple linear regression model is attractive because it is linear with respect to $w$ but has severe limitations because it is also linear with respect to x. These greatly limits the kinds of predictions that this model will be able to make. However, the introduction of \emph{basis functions}, shown as $\phi$ above, goes some way towards solving this problem. The introduction of a non-linear basis function means that models can be made non-linear functions of input $x$ but remain linear in $w$ which makes them computationally easier to solve.
		
		Students might give the example of polynomial regression in which ${\phi}_j(x)=x^j$ or some other suitable example.
		\end{answer}
		


\item A multivariate logistic regression model has been built to predict the propensity of shoppers to perform a repeat purchase of a free gift that they are given. The descriptive features used by the model are the age of the customer, the average amount of money the customer spends on each visit to the shop, and the average number of visits the customer makes to the shop per week. This model is being used by the marketing department to determine who should be given the free gift. The trained model is
\begin{alignat*}{2}
\textsc{Repeat~Purchase} = & -3.82398 - 0.02990   \times \textsc{Age} \\ 
& + 0.74572 \times \textsc{Shop~Frequency}\\
& + 0.02999 \times \textsc{Shop~Value}
\end{alignat*}
And, the logistic function is defined as:
\begin{equation*}
	logistic(x)=\frac{1}{1+e^{-x}}
\end{equation*}
Assuming that the \textit{yes} level is the positive level and the classification threshold is $0.5$, use this model to make predictions for each of the query instances shown in Table \ref{tab:logregress} below.
\marks{18}

%\begin{footnotesize}
%{\setlength{\tabcolsep}{0.1em}
%\noindent\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}} l r r r@{}}
\begin{table}[!hb]
\caption{The queries for the multivariate logistic regression question}
\label{tab:logregress}
\begin{center}
\begin{tabular}{l r r r}
\hline
			 & ~ & \textsc{Shop}  & \textsc{Shop}\\
				\textsc{ID} & \textsc{Age} & \textsc{Frequency}  & \textsc{Value}\\
\hline
1 & 56 & 1.60 & 109.32 \\ 
2 & 21 & 4.92 & 11.28 \\  
3 & 48 & 1.21 & 161.19 \\ 
%4 & 37 & 0.72 & 170.65 \\ 
%5 & 32 & 1.08 & 165.39 \\ 
\hline
\end{tabular}
\end{center}
\end{table}
%\end{tabular*}{}
%}
%\end{footnotesize}
%\end{adjustwidth}
		\begin{answer}
		Calculating the predictions made by the model simply involves inserting the descriptive features from each query instance into the prediction model. With this information, the predictions can be made as follows:
				
		\begin{description}
		\item[1:] $Logistic(-3.82398+-0.0299\times56+0.74572\times1.6+0.02999\times109.32)$ \\
		$=Logistic(-1.02672) =\frac{1}{1-e^{1.02672}}$\\
		$=0.26372 \Rightarrow \textit{no}$
		\item[2:] $Logistic(-3.82398+-0.0299\times21+0.74572\times4.92+0.02999\times11.28)$ \\
		$=Logistic(-0.44465)  =\frac{1}{1-e^{0.44465}}$ \\
		$=0.390633 \Rightarrow \textit{no}$
		\item[3:] $Logistic(-3.82398+-0.0299\times48+0.74572\times1.21+0.02999\times161.19)$ \\
		$=Logistic(0.477229)   =\frac{1}{1-e^{-0.477229}}$ \\
		$=0.6205 \Rightarrow \textit{yes}$
%		\item[4:] $Logistic(-3.82398+-0.0299\times37+0.74572\times0.72+0.02999\times170.65)$ \\
%		$=Logistic(0.724432)   =\frac{1}{1-e^{0.-724432}}$ \\
%		$=0.673582 \Rightarrow \textit{yes}$
%		\item[5:] $Logistic(-3.82398+-0.0299\times32+0.74572\times1.08+0.02999\times165.39)$ \\
%		$=Logistic(0.984644)   =\frac{1}{1-e^{-0.98464}}$ \\
%		$=0.728029 \Rightarrow \textit{yes}$
		\end{description}
		\end{answer}
\end{enumerate}







\end{document}
